{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from my_ppo_2 import PPO\n",
    "from SAC import SAC\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self,buffer_size,state_dim,action_dim):\n",
    "        self.buffer_size=buffer_size\n",
    "        self.index=0\n",
    "        self.state=np.zeros((buffer_size,state_dim))\n",
    "        self.action=np.zeros((buffer_size,action_dim))\n",
    "        self.next_state=np.zeros((buffer_size,state_dim))\n",
    "        self.reward=np.zeros(buffer_size,dtype=float)\n",
    "        self.done=np.zeros(buffer_size,dtype=bool)\n",
    "        \n",
    "    def store(self,state,action,next_state,done):\n",
    "        index=self.index%self.buffer_size\n",
    "        self.state[index]=state\n",
    "        self.action[index]=action\n",
    "        self.next_state[index]=next_state\n",
    "        self.reward[index]=0.0\n",
    "        self.done[index]=done\n",
    "        self.index+=1\n",
    "        \n",
    "    def sample(self,batch_size,return_index=False):\n",
    "        sample_index=np.random.choice(batch_size,self.buffer_size,replace=True)\n",
    "        if return_index:\n",
    "            return sample_index\n",
    "        state=self.state[sample_index]\n",
    "        action=self.action[sample_index]\n",
    "        next_state=self.next_state[sample_index]\n",
    "        reward=self.reward[sample_index]\n",
    "        done=self.done[sample_index]\n",
    "        return state,action,next_state,reward,done\n",
    "    \n",
    "    def clean(self):\n",
    "        self.index=0\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomEnv:\n",
    "    def __init__(self,env_id):\n",
    "        self.env=gym.make(env_id)\n",
    "        self.replay_buffer=ReplayBuffer(8192,self.env.observation_space.shape[0],self.env.action_space.shape[0])\n",
    "        \n",
    "    def reset(self):\n",
    "        return self.env.reset()\n",
    "    \n",
    "    def step(self,state,action):\n",
    "        state_,reward,done,info=self.env.step(action)\n",
    "        self.replay_buffer.store(state,action,state_,done)\n",
    "        return state_,reward,done,info\n",
    "    \n",
    "    def get_state_dim(self):\n",
    "        return self.env.observation_space.shape\n",
    "    \n",
    "    def get_action_dim(self):\n",
    "        return self.env.action_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StudentAgent:\n",
    "    def __init__(self,state_dim,action_dim):\n",
    "        self.model=PPO(state_dim,action_dim,1e-3,1e-3,0.99,80,0.2,True)\n",
    "        \n",
    "    def update(self,replay_buffer,batch_size):\n",
    "        index=replay_buffer.sample(batch_size,True)\n",
    "        self.model.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_env=CustomEnv('Pendulum-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "d=False\n",
    "s=custom_env.reset()\n",
    "i=0\n",
    "while not d:\n",
    "    a=custom_env.env.action_space.sample()\n",
    "    s_,r,d,_=custom_env.step(s,a)\n",
    "    s=s_\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=custom_env.replay_buffer.sample(64,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sa=StudentAgent(custom_env.get_state_dim()[0],custom_env.get_action_dim()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.95834779 -0.28560377 -0.32741271]\n",
      " [-0.99808779  0.06181238 -1.321099  ]\n",
      " [-0.99977756 -0.0210911   0.93504803]\n",
      " ...\n",
      " [-0.99907628 -0.04297188  1.03695861]\n",
      " [-0.93054645 -0.36617386  0.17125319]\n",
      " [-0.9760131  -0.21771181  1.28945615]]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for dimension 1 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/tools/DR/MyProject/RILE/playground.ipynb 单元格 10\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/tools/DR/MyProject/RILE/playground.ipynb#X12sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m sa\u001b[39m.\u001b[39;49mupdate(custom_env\u001b[39m.\u001b[39;49mreplay_buffer,\u001b[39m64\u001b[39;49m)\n",
      "\u001b[1;32m/home/tools/DR/MyProject/RILE/playground.ipynb 单元格 10\u001b[0m line \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/tools/DR/MyProject/RILE/playground.ipynb#X12sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m index\u001b[39m=\u001b[39mreplay_buffer\u001b[39m.\u001b[39msample(batch_size,\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/tools/DR/MyProject/RILE/playground.ipynb#X12sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mprint\u001b[39m(replay_buffer\u001b[39m.\u001b[39mstate[index])\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/tools/DR/MyProject/RILE/playground.ipynb#X12sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mtrain(replay_buffer\u001b[39m.\u001b[39;49mstate[index],replay_buffer\u001b[39m.\u001b[39;49maction[index],replay_buffer\u001b[39m.\u001b[39;49mreward[index],[\u001b[39m0\u001b[39;49m])\n",
      "File \u001b[0;32m/home/tools/DR/MyProject/RILE/PPO.py:39\u001b[0m, in \u001b[0;36mPPOAgent.train\u001b[0;34m(self, states, actions, rewards, values)\u001b[0m\n\u001b[1;32m     36\u001b[0m advantages \u001b[39m=\u001b[39m rewards \u001b[39m-\u001b[39m values\n\u001b[1;32m     38\u001b[0m \u001b[39m# 计算策略梯度\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m log_probs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mlog(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mactor_critic(states)[\u001b[39mrange\u001b[39;49m(\u001b[39mlen\u001b[39;49m(states)), actions])\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m     40\u001b[0m actor_loss \u001b[39m=\u001b[39m \u001b[39m-\u001b[39mtorch\u001b[39m.\u001b[39mmean(log_probs \u001b[39m*\u001b[39m advantages)\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m     42\u001b[0m \u001b[39m# 计算价值函数损失\u001b[39;00m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 1 is out of bounds for dimension 1 with size 1"
     ]
    }
   ],
   "source": [
    "sa.update(custom_env.replay_buffer,64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=custom_env.replay_buffer.sample(64,return_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "s=torch.FloatTensor(custom_env.replay_buffer.state[i])\n",
    "a=torch.FloatTensor(custom_env.replay_buffer.action[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=custom_env.replay_buffer.sample(10,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.97553698,  0.21983537, -0.75659917],\n",
       "       [-0.9831534 ,  0.18278237, -0.82048096],\n",
       "       [-0.95259862,  0.30422996, -0.23870135],\n",
       "       ...,\n",
       "       [-0.99976169,  0.02183023, -0.77791089],\n",
       "       [-0.99976169,  0.02183023, -0.77791089],\n",
       "       [-0.96535131,  0.26095373, -0.84728643]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_env.replay_buffer.state[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(i[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
