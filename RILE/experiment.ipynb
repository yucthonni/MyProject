{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Deprecation warnings have been disabled. Set TF_ENABLE_DEPRECATION_WARNINGS=1 to re-enable them.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-30 09:56:16.725938: I tensorflow/stream_executor/platform/default/dso_loader.cc:50] Successfully opened dynamic library libcudart.so.12\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from buffer import ReplayBuffer\n",
    "from PPO import PPO\n",
    "import gym\n",
    "from Discriminator import Discriminator\n",
    "from reward_env import RewardEnv\n",
    "\n",
    "import contextlib\n",
    "import network_sim\n",
    "from stable_baselines3 import PPO as sb3ppo\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device='cuda'\n",
    "BUFFER_SIZE=81920\n",
    "hidden_dim=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CustomEnv:\n",
    "    def __init__(self,env_id):\n",
    "        with contextlib.redirect_stderr(None):\n",
    "            self.env=gym.make(env_id)\n",
    "        self.student_buffer=ReplayBuffer(BUFFER_SIZE)\n",
    "        self.teacher_buffer=ReplayBuffer(BUFFER_SIZE)\n",
    "        \n",
    "    def reset(self):\n",
    "        with contextlib.redirect_stderr(None),contextlib.redirect_stdout(None):\n",
    "            return self.env.reset()\n",
    "    \n",
    "    def step(self,action):\n",
    "        with contextlib.redirect_stderr(None),contextlib.redirect_stdout(None):\n",
    "            return self.env.step(action)\n",
    "    \n",
    "    def get_state_dim(self):\n",
    "        return self.env.observation_space.shape\n",
    "    \n",
    "    def get_action_dim(self):\n",
    "        return self.env.action_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpertTrajectory(ReplayBuffer):\n",
    "    def __init__(self, buffer_size):\n",
    "        super().__init__(buffer_size)\n",
    "        self.model=None\n",
    "        self.max_step=buffer_size*5\n",
    "        \n",
    "    def load_expert_model(self,path:str):\n",
    "        self.model=sb3ppo.load(path)\n",
    "    \n",
    "    def generate_trajectory(self,env:CustomEnv,max_step=0):\n",
    "        if max_step:\n",
    "            self.max_step=max_step\n",
    "        with tqdm(range(self.max_step)) as pb:\n",
    "            while True:\n",
    "                s=env.reset()\n",
    "                d=False\n",
    "                while not d:\n",
    "                    a,_=self.model.predict(s)\n",
    "                    s_,r,d,_=env.step(a)\n",
    "                    self.store(s,a,None,s_,r,None,d)\n",
    "                    s=s_\n",
    "                    pb.update()\n",
    "                if pb.n>=self.max_step:\n",
    "                    break\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "History length: 10\n",
      "Features: ['sent latency inflation', 'latency ratio', 'send ratio']\n",
      "Getting min obs for ['sent latency inflation', 'latency ratio', 'send ratio']\n"
     ]
    }
   ],
   "source": [
    "env=CustomEnv('PccNs-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "et=ExpertTrajectory(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "et.load_expert_model('verygood.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400it [00:00, 436.81it/s]                     \n"
     ]
    }
   ],
   "source": [
    "et.generate_trajectory(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StudentAgent:\n",
    "    def __init__(self,state_dim,action_dim,env:CustomEnv):\n",
    "        self.env=env\n",
    "        self.replay_buffer=env.student_buffer\n",
    "        self.model=PPO(state_dim,action_dim,env.student_buffer)\n",
    "        \n",
    "    def generate_trajectory(self,step:int):\n",
    "        pb=tqdm(range(step))\n",
    "        num=0\n",
    "        for i in pb:\n",
    "            s=self.env.reset()\n",
    "            d=False\n",
    "            while not d:\n",
    "                a,_,l,v=self.model.select_action(s)\n",
    "                s_,r,d,_=self.env.step(a)\n",
    "                self.model.buffer.store(s,a,l,s_,r,v,d)\n",
    "                s=s_\n",
    "                num+=1\n",
    "            #pb.update()\n",
    "        print('生成',num,'条轨迹')   \n",
    "        \n",
    "    def train(self,total_timestep,batch_size):\n",
    "        pb=tqdm(range(total_timestep))\n",
    "        for i in pb:\n",
    "            self.model.update(batch_size)\n",
    "            pb.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "sa=StudentAgent(np.prod(env.get_state_dim()),env.get_action_dim()[0],env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sa.model=torch.load('model/student/studentmodel.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:02<00:00,  3.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "生成 3200 条轨迹\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sa.generate_trajectory(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:36<00:00,  2.72it/s]\n"
     ]
    }
   ],
   "source": [
    "sa.train(100,2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TeacherAgent():\n",
    "    def __init__(self,state_dim,action_dim,env:CustomEnv,expert_trajectory):\n",
    "        self.trajectory_buffer=env.student_buffer\n",
    "        self.replay_buffer=env.teacher_buffer\n",
    "        self.model=PPO(state_dim+action_dim,1,self.replay_buffer)\n",
    "        self.discriminator=Discriminator(state_dim+action_dim,hidden_dim,64,self.trajectory_buffer,expert_trajectory)\n",
    "        \n",
    "    def ComputeReward(self):\n",
    "        pb=tqdm(range(min(self.trajectory_buffer.index,self.trajectory_buffer.buffer_size)))\n",
    "        for i in pb:\n",
    "            sa_pair=torch.cat((self.trajectory_buffer.state[i],self.trajectory_buffer.action[i]),-1)\n",
    "            reward,_,l,v=self.model.select_action(sa_pair)\n",
    "            self.replay_buffer.store(sa_pair,\n",
    "                                     reward,\n",
    "                                     l,\n",
    "                                     self.trajectory_buffer.next_state[i],\n",
    "                                     self.discriminator.model(sa_pair).detach().cpu().numpy(),\n",
    "                                     v,\n",
    "                                     self.trajectory_buffer.done[i],\n",
    "                                     )\n",
    "            self.trajectory_buffer.reward[i]=reward\n",
    "            pb.update()\n",
    "        self.discriminator.collect_expert()\n",
    "            \n",
    "    def trainPPO(self,total_timestep:int):\n",
    "        for i in range(total_timestep):\n",
    "            self.model.update(1024)\n",
    "            \n",
    "    def trainDiscriminator(self,total_timestep:int):\n",
    "        self.discriminator.update(total_timestep,False)\n",
    "        \n",
    "    def train(self,total_timestep:int,PPO_timestep:int,D_timestep:int):\n",
    "        pb=tqdm(range(total_timestep))\n",
    "        for i in pb:\n",
    "            print('Computing reward...')\n",
    "            self.ComputeReward()\n",
    "            print('Training Discriminator...')\n",
    "            self.trainDiscriminator(D_timestep)\n",
    "            print('Training PPO...')\n",
    "            self.trainPPO(PPO_timestep)\n",
    "            pb.update()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ta=TeacherAgent(np.prod(env.get_state_dim()),env.get_action_dim()[0],env,et)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.student_buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sa.replay_buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sa.model.buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ta.trajectory_buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "ta.ComputeReward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "a must be greater than 0 unless no samples are taken",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/tools/DR/MyProject/RILE/experiment.ipynb 单元格 21\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/tools/DR/MyProject/RILE/experiment.ipynb#Y151sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m ta\u001b[39m.\u001b[39;49mtrainDiscriminator(\u001b[39m100\u001b[39;49m)\n",
      "\u001b[1;32m/home/tools/DR/MyProject/RILE/experiment.ipynb 单元格 21\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tools/DR/MyProject/RILE/experiment.ipynb#Y151sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrainDiscriminator\u001b[39m(\u001b[39mself\u001b[39m,total_timestep:\u001b[39mint\u001b[39m):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/tools/DR/MyProject/RILE/experiment.ipynb#Y151sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdiscriminator\u001b[39m.\u001b[39;49mupdate(total_timestep,\u001b[39mFalse\u001b[39;49;00m)\n",
      "File \u001b[0;32m/home/tools/DR/MyProject/RILE/Discriminator.py:58\u001b[0m, in \u001b[0;36mDiscriminator.update\u001b[0;34m(self, total_timestep, progress)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     57\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(total_timestep):\n\u001b[0;32m---> 58\u001b[0m         stu_traj\u001b[39m=\u001b[39m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollect_dist(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_size)\n\u001b[1;32m     59\u001b[0m         eo\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexpert_dist)\n\u001b[1;32m     60\u001b[0m         so\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpolicy_dist)\n",
      "File \u001b[0;32m/home/tools/DR/MyProject/RILE/Discriminator.py:37\u001b[0m, in \u001b[0;36mDiscriminator.collect_dist\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcollect_dist\u001b[39m(\u001b[39mself\u001b[39m,batch_size):\n\u001b[0;32m---> 37\u001b[0m     index\u001b[39m=\u001b[39m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msbuffer\u001b[39m.\u001b[39;49msample(batch_size,\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     38\u001b[0m     state\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mstack([\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msbuffer\u001b[39m.\u001b[39mstate[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m index],dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m     39\u001b[0m     action\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mstack([\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msbuffer\u001b[39m.\u001b[39maction[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m index],dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m/home/tools/DR/MyProject/RILE/buffer.py:48\u001b[0m, in \u001b[0;36mReplayBuffer.sample\u001b[0;34m(self, batch_size, return_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msample\u001b[39m(\u001b[39mself\u001b[39m,batch_size,return_index\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m---> 48\u001b[0m     sample_index\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39;49mrandom\u001b[39m.\u001b[39;49mchoice(\u001b[39mmin\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbuffer_size,\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindex),batch_size,replace\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     49\u001b[0m     \u001b[39mif\u001b[39;00m return_index:\n\u001b[1;32m     50\u001b[0m         \u001b[39mreturn\u001b[39;00m sample_index\n",
      "File \u001b[0;32mmtrand.pyx:909\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: a must be greater than 0 unless no samples are taken"
     ]
    }
   ],
   "source": [
    "ta.trainDiscriminator(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ta.trainPPO(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第 0 次更新\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1024 [00:00<?, ?it/s]/home/tools/DR/MyProject/RILE/PPO.py:184: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:261.)\n",
      "  rewards = torch.tensor(rewards, dtype=torch.float32).to(device)\n",
      "/home/data/envs/IL/lib/python3.8/site-packages/torch/nn/modules/loss.py:535: UserWarning: Using a target size (torch.Size([4096, 1])) that is different to the input size (torch.Size([4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "  0%|          | 4/1024 [00:05<22:25,  1.32s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/tools/DR/MyProject/RILE/experiment.ipynb 单元格 23\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/tools/DR/MyProject/RILE/experiment.ipynb#Y153sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m10\u001b[39m):\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/tools/DR/MyProject/RILE/experiment.ipynb#Y153sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m第\u001b[39m\u001b[39m'\u001b[39m,i,\u001b[39m'\u001b[39m\u001b[39m次更新\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/tools/DR/MyProject/RILE/experiment.ipynb#Y153sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     sa\u001b[39m.\u001b[39;49mtrain(\u001b[39m1024\u001b[39;49m,\u001b[39m4096\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/tools/DR/MyProject/RILE/experiment.ipynb#Y153sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     ta\u001b[39m.\u001b[39mtrain(\u001b[39m10\u001b[39m,\u001b[39m100\u001b[39m,\u001b[39m100\u001b[39m)\n",
      "\u001b[1;32m/home/tools/DR/MyProject/RILE/experiment.ipynb 单元格 23\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tools/DR/MyProject/RILE/experiment.ipynb#Y153sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m pb\u001b[39m=\u001b[39mtqdm(\u001b[39mrange\u001b[39m(total_timestep))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tools/DR/MyProject/RILE/experiment.ipynb#Y153sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m pb:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/tools/DR/MyProject/RILE/experiment.ipynb#Y153sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mupdate(batch_size)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tools/DR/MyProject/RILE/experiment.ipynb#Y153sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     pb\u001b[39m.\u001b[39mupdate()\n",
      "File \u001b[0;32m/home/tools/DR/MyProject/RILE/PPO.py:204\u001b[0m, in \u001b[0;36mPPO.update\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[39m# Optimize policy for K epochs\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mK_epochs):\n\u001b[1;32m    202\u001b[0m \n\u001b[1;32m    203\u001b[0m     \u001b[39m# Evaluating old actions and values\u001b[39;00m\n\u001b[0;32m--> 204\u001b[0m     logprobs, state_values, dist_entropy \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpolicy\u001b[39m.\u001b[39;49mevaluate(old_states, old_actions)\n\u001b[1;32m    206\u001b[0m     \u001b[39m# match state_values tensor dimensions with rewards tensor\u001b[39;00m\n\u001b[1;32m    207\u001b[0m     state_values \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39msqueeze(state_values)\n",
      "File \u001b[0;32m/home/tools/DR/MyProject/RILE/PPO.py:110\u001b[0m, in \u001b[0;36mActorCritic.evaluate\u001b[0;34m(self, state, action)\u001b[0m\n\u001b[1;32m    108\u001b[0m action_var\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_var\u001b[39m.\u001b[39mexpand_as(action_mean)\n\u001b[1;32m    109\u001b[0m cov_mat\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mdiag_embed(action_var)\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m--> 110\u001b[0m dist\u001b[39m=\u001b[39mMultivariateNormal(action_mean,cov_mat)\n\u001b[1;32m    111\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_dim\u001b[39m==\u001b[39m\u001b[39m1\u001b[39m:\n\u001b[1;32m    112\u001b[0m     action\u001b[39m=\u001b[39maction\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,\u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_dim)\n",
      "File \u001b[0;32m/home/data/envs/IL/lib/python3.8/site-packages/torch/distributions/multivariate_normal.py:177\u001b[0m, in \u001b[0;36mMultivariateNormal.__init__\u001b[0;34m(self, loc, covariance_matrix, precision_matrix, scale_tril, validate_args)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloc \u001b[39m=\u001b[39m loc\u001b[39m.\u001b[39mexpand(batch_shape \u001b[39m+\u001b[39m (\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,))\n\u001b[1;32m    176\u001b[0m event_shape \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloc\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m:]\n\u001b[0;32m--> 177\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(batch_shape, event_shape, validate_args\u001b[39m=\u001b[39;49mvalidate_args)\n\u001b[1;32m    179\u001b[0m \u001b[39mif\u001b[39;00m scale_tril \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    180\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_unbroadcasted_scale_tril \u001b[39m=\u001b[39m scale_tril\n",
      "File \u001b[0;32m/home/data/envs/IL/lib/python3.8/site-packages/torch/distributions/distribution.py:67\u001b[0m, in \u001b[0;36mDistribution.__init__\u001b[0;34m(self, batch_shape, event_shape, validate_args)\u001b[0m\n\u001b[1;32m     65\u001b[0m         value \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, param)\n\u001b[1;32m     66\u001b[0m         valid \u001b[39m=\u001b[39m constraint\u001b[39m.\u001b[39mcheck(value)\n\u001b[0;32m---> 67\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m valid\u001b[39m.\u001b[39mall():\n\u001b[1;32m     68\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m     69\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExpected parameter \u001b[39m\u001b[39m{\u001b[39;00mparam\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     70\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(value)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m of shape \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtuple\u001b[39m(value\u001b[39m.\u001b[39mshape)\u001b[39m}\u001b[39;00m\u001b[39m) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     73\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut found invalid values:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mvalue\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     74\u001b[0m             )\n\u001b[1;32m     75\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print('第',i,'次更新')\n",
    "    sa.train(1024,4096)\n",
    "    ta.train(10,100,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards=[]\n",
    "for i in range(100):\n",
    "    s=env.reset()\n",
    "    d=False\n",
    "    reward=0\n",
    "    while not d:\n",
    "        a,_,_,_=sa.model.select_action(s)\n",
    "        s,r,d,_=env.step(a)\n",
    "        reward+=r\n",
    "    rewards.append(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "440.69136894767644"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards=[]\n",
    "for i in range(100):\n",
    "    s=env.reset()\n",
    "    d=False\n",
    "    reward=0\n",
    "    while not d:\n",
    "        a,_=et.model.predict(s)\n",
    "        s_,r,d,_=env.step(a)\n",
    "        reward+=r\n",
    "        s=s_\n",
    "    rewards.append(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "608.6122610776468"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(rewards)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
