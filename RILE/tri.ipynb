{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import network_sim\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "from specbuffer import SpecReplayBuffer\n",
    "from PPO import PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpecReplayBuffer:\n",
    "    def __init__(self,buffer_size):\n",
    "        self.buffer_size=buffer_size\n",
    "        self.index=0\n",
    "        self.state=[]\n",
    "        self.action=[]\n",
    "        self.logprobs=[]\n",
    "        self.next_state=[]\n",
    "        self.reward=[]\n",
    "        self.state_value=[]\n",
    "        self.done=[]\n",
    "        self.teacher_reward=torch.zeros(buffer_size)\n",
    "        self.islatest=False\n",
    "        self.state_buffer=[]\n",
    "        self.isAfter=False\n",
    "        \n",
    "        \n",
    "    def store(self,state,action,log_prob=None,next_state=None,reward=None,state_value=None,done=None):\n",
    "        #buffer_size必须是400的倍数\n",
    "        self.state_buffer.append(state)\n",
    "        \n",
    "        next_state=torch.FloatTensor(next_state).view(-1)\n",
    "        action=torch.FloatTensor(action)\n",
    "        reward=torch.FloatTensor(np.array(reward))\n",
    "        if self.index>=self.buffer_size:\n",
    "            index=self.index%self.buffer_size\n",
    "            if done:\n",
    "                self.state[int(index/400)]=self.state_buffer\n",
    "                self.state_buffer=[]\n",
    "            self.action[index]=action\n",
    "            self.logprobs[index]=log_prob\n",
    "            self.next_state[index]=next_state\n",
    "            self.reward[index]=reward\n",
    "            self.state_value[index]=state_value\n",
    "            self.done[index]=done\n",
    "        else:\n",
    "            if done:\n",
    "                self.state.append(self.state_buffer)\n",
    "                self.state_buffer=[]\n",
    "            self.action.append(action)\n",
    "            self.logprobs.append(log_prob)\n",
    "            self.next_state.append(next_state)\n",
    "            self.reward.append(reward)\n",
    "            self.state_value.append(state_value)\n",
    "            self.done.append(done)\n",
    "        self.index+=1\n",
    "        \n",
    "        \n",
    "    def sample(self,batch_size,return_index=False):\n",
    "        sample_index=np.random.choice(min(self.buffer_size,self.index),batch_size,replace=True)\n",
    "        if return_index:\n",
    "            return sample_index\n",
    "        length=len(self.state[0])\n",
    "        state=[torch.FloatTensor(self.get_state_padding(self.state[int(i/length)],i%length,10,True)[0]).view(-1) for i in sample_index]\n",
    "        action=[self.action[i] for i in sample_index]\n",
    "        log_prob=[self.logprobs[i] for i in sample_index]\n",
    "        next_state=[self.next_state[i] for i in sample_index]\n",
    "        reward=[self.reward[i] for i in sample_index]\n",
    "        state_value=[self.state_value[i] for i in sample_index]\n",
    "        done=[self.done[i] for i in sample_index]\n",
    "        teacher_reward=[self.teacher_reward[i] for i in sample_index]\n",
    "        return state,action,log_prob,next_state,reward,state_value,done,teacher_reward\n",
    "    \n",
    "    def get_state_padding(self,a:list,index:int,offset:int,zero_padding:bool):\n",
    "        if not zero_padding:\n",
    "            # return a[max(0,index-offset):min(index+offset,len(a))]\n",
    "            return a[max(0,index-offset+1):index+1],a[index+1:min(index+offset+1,len(a)+1)]\n",
    "        b=a.copy()\n",
    "        zero_padding=np.copy(b[0])\n",
    "        for _ in range(offset):\n",
    "            b.insert(0,zero_padding)\n",
    "            b.append(zero_padding)\n",
    "        # return b[max(0,index):min(index+2*offset,len(a)+2*offset)]\n",
    "        return b[max(0,index+1):index+offset+1],b[index+offset+1:min(index+2*offset+1,len(a)+2*offset+1)]\n",
    "    \n",
    "    def get_reward_padding(self,a:list,index:int,offset:int,interval:int,zero_padding:bool):\n",
    "        low=int(index/interval)*interval\n",
    "        high=(int(index/interval)+1)*interval\n",
    "        if not zero_padding:\n",
    "            # return a[max(0,index-offset):min(index+offset,len(a))]\n",
    "            return a[max(low,index-offset+1):index+1],a[index+1:min(index+offset+1,high)]\n",
    "        b=a.copy()\n",
    "        zero_padding=b[low]\n",
    "        for _ in range(offset):\n",
    "        # b.insert(low,zero_padding)\n",
    "            b.insert(high,zero_padding)\n",
    "        for _ in range(offset):\n",
    "            b.insert(low,zero_padding)\n",
    "        # return b[max(0,index):min(index+2*offset,len(a)+2*offset)]\n",
    "        return b[max(low,index+1):index+offset+1],b[index+offset+1:min(index+2*offset+1,high+2*offset)]\n",
    "    \n",
    "    \n",
    "    def state_from_index(self,index:list):\n",
    "        if self.isAfter:\n",
    "            return self.from_after_state(index)\n",
    "        else:\n",
    "            return self.from_before_state(index)\n",
    "    \n",
    "    def from_before_state(self,index:list):\n",
    "        length=len(self.state[0])\n",
    "        state=[torch.FloatTensor(np.array(self.get_state_padding(self.state[int(i/length)],i%length,10,True)[0])).view(-1) for i in index]\n",
    "        return torch.stack(state,dim=0).detach()\n",
    "    \n",
    "    def from_after_state(self,index:list):\n",
    "        length=len(self.state[0])\n",
    "        state=[torch.FloatTensor(np.array(self.get_state_padding(self.state[int(i/length)],i%length,10,True)[1])).view(-1) for i in index]\n",
    "        return torch.stack(state,dim=0).detach()\n",
    "    \n",
    "    def from_after_reward(self,index:list):\n",
    "        length=len(self.state[0])\n",
    "        state=[torch.FloatTensor(np.array(self.get_reward_padding(self.reward,i,10,length,True)[1])).view(-1) for i in index]\n",
    "        return torch.stack(state,dim=0).detach()\n",
    "        \n",
    "    def clean(self):#这里暂时有个bug，不能解决append那里，不过暂时用不上这个函数，就先不管了\n",
    "        self.index=0\n",
    "        self.state=[]\n",
    "        self.action=[]\n",
    "        self.logprobs=[]\n",
    "        self.next_state=[]\n",
    "        self.reward=[]\n",
    "        self.state_value=[]\n",
    "        self.done=[]\n",
    "        self.teacher_reward=[]\n",
    "        \n",
    "    def setAfter(self,isAfter:bool):\n",
    "        self.isAfter=isAfter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "History length: 10\n",
      "Features: ['sent latency inflation', 'latency ratio', 'send ratio']\n",
      "Getting min obs for ['sent latency inflation', 'latency ratio', 'send ratio']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuc/.local/lib/python3.8/site-packages/gym/spaces/box.py:127: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "env=gym.make('PccNs-v0')\n",
    "replaybuffer=SpecReplayBuffer(1600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=PPO(30,1,replaybuffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yuc/.local/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:174: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/yuc/.local/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:187: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/yuc/.local/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:195: UserWarning: \u001b[33mWARN: The result returned by `env.reset()` was not a tuple of the form `(obs, info)`, where `obs` is a observation and `info` is a dictionary containing additional information. Actual type: `<class 'numpy.ndarray'>`\u001b[0m\n",
      "  logger.warn(\n",
      "/home/yuc/.local/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:219: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
      "  logger.deprecation(\n",
      "/home/yuc/.local/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:141: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method was expecting numpy array dtype to be float32, actual type: float64\u001b[0m\n",
      "  logger.warn(\n",
      "/home/yuc/.local/lib/python3.8/site-packages/gym/utils/passive_env_checker.py:165: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward: 0.00, Ewma Reward: 0.00\n",
      "Reward: 192.45, Ewma Reward: 1.92\n",
      "Reward: -72.28, Ewma Reward: 1.18\n",
      "Reward: 111.67, Ewma Reward: 2.29\n",
      "Reward: 268.57, Ewma Reward: 4.95\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    s=env.reset()\n",
    "    d=False\n",
    "    while not d:\n",
    "        a,action,l,v=model.select_action(s)\n",
    "        s_,r,d,_=env.step(a)\n",
    "        replaybuffer.store(s[-3:],a,l,s_,r,v,d)\n",
    "        s=s_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.5535,  1.5902,  1.6580,  1.7017,  1.7142,  1.6959,  1.6786,  1.7096,\n",
       "          1.7771,  1.7655],\n",
       "        [ 0.0912,  0.1404,  0.0691,  0.0296, -0.0024,  0.1644,  0.1390, -0.0394,\n",
       "          0.0940,  0.1136],\n",
       "        [ 1.5556,  1.5556,  1.5556,  1.5556,  1.5556,  1.5556,  1.5556,  1.5556,\n",
       "          1.5556,  1.5556]])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replaybuffer.from_after_reward([1,300,399])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.update(1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def get_state_padding(a:list,index:int,offset:int,zero_padding:bool):\n",
    "    if not zero_padding:\n",
    "        # return a[max(0,index-offset):min(index+offset,len(a))]\n",
    "        return a[max(0,index-offset+1):index+1],a[index+1:min(index+offset+1,len(a)+1)]\n",
    "    b=a.copy()\n",
    "    zero_padding=np.copy(b[0])\n",
    "    for _ in range(offset):\n",
    "        b.insert(0,zero_padding)\n",
    "        b.append(zero_padding)\n",
    "    # return b[max(0,index):min(index+2*offset,len(a)+2*offset)]\n",
    "    return b[max(0,index+1):index+offset+1],b[index+offset+1:min(index+2*offset+1,len(a)+2*offset+1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def get_state(a:list,index:int,offset:int):\n",
    "    return a[max(0,index-offset):min(index+offset,len(a))]\n",
    "    # return max(0,index-offset+1),min(index+offset+1,len(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy=model.policy.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(policy.state_dict(),'modelcpu.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['actor.0.weight', 'actor.0.bias', 'actor.2.weight', 'actor.2.bias', 'actor.4.weight', 'actor.4.bias', 'critic.0.weight', 'critic.0.bias', 'critic.2.weight', 'critic.2.bias', 'critic.4.weight', 'critic.4.bias'])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.state_dict().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "load_state_dict() missing 1 required positional argument: 'state_dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/tools/DR/MyProject/RILE/tri.ipynb 单元格 31\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/tools/DR/MyProject/RILE/tri.ipynb#X42sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model\u001b[39m.\u001b[39;49mpolicy\u001b[39m.\u001b[39;49mload_state_dict()\n",
      "\u001b[0;31mTypeError\u001b[0m: load_state_dict() missing 1 required positional argument: 'state_dict'"
     ]
    }
   ],
   "source": [
    "model.policy.load_state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state_padding(a:list,index:int,offset:int,interval:int,zero_padding:bool):\n",
    "    low=int(index/interval)*interval\n",
    "    high=(int(index/interval)+1)*interval\n",
    "    if not zero_padding:\n",
    "        # return a[max(0,index-offset):min(index+offset,len(a))]\n",
    "        return a[max(low,index-offset+1):index+1],a[index+1:min(index+offset+1,high)]\n",
    "    b=a.copy()\n",
    "    zero_padding=b[0]\n",
    "    for _ in range(offset):\n",
    "    # b.insert(low,zero_padding)\n",
    "        b.insert(high,zero_padding)\n",
    "    for _ in range(offset):\n",
    "        b.insert(low,zero_padding)\n",
    "    # return b[max(0,index):min(index+2*offset,len(a)+2*offset)]\n",
    "    return b[max(low,index+1):index+offset+1],b[index+offset+1:min(index+2*offset+1,high+2*offset)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "l=[i for i in range(1600)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "l.insert(5,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 0, 0, 0, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([385, 386, 387, 388, 389, 390, 391, 392, 393, 394, 395, 396, 397, 398, 399],\n",
       " [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_state_padding(l,399,15,400,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 1, 2, 3, 4, 5, 6, 7], [8, 9, 10])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "offset=10\n",
    "max=20\n",
    "index=47\n",
    "low=int(index/max)*max\n",
    "high=int()\n",
    "l[max(low,index-offset+1):index+1],l[index+1:min(index+offset+1,high+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "from specbuffer import SpecReplayBuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(SpecReplayBuffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env=gym.make('PccNs-v0')\n",
    "replaybuffer=ReplayBuffer(1600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[os.path.splitext(os.path.basename(file))[0] for file in filelist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "path='/home/tools/DR/MyProject/RILE/model/student/deeptrain/studentmodel0855.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import MultivariateNormal\n",
    "from torch.distributions import Categorical\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard.writer import SummaryWriter\n",
    "\n",
    "################################## set device ##################################\n",
    "device = torch.device('cpu')\n",
    "if (torch.cuda.is_available()):\n",
    "    device = torch.device('cuda:0')\n",
    "    torch.cuda.empty_cache()\n",
    "debug=False\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim, has_continuous_action_space, action_std_init):\n",
    "        super(ActorCritic, self).__init__()\n",
    "\n",
    "        self.has_continuous_action_space = has_continuous_action_space\n",
    "\n",
    "        if has_continuous_action_space:\n",
    "            self.action_dim = action_dim\n",
    "            self.action_var = torch.full(\n",
    "                (action_dim,), action_std_init * action_std_init).to(device)\n",
    "        # actor\n",
    "        if has_continuous_action_space:\n",
    "            self.actor = nn.Sequential(\n",
    "                nn.Linear(state_dim, hidden_dim),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(hidden_dim, hidden_dim),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(hidden_dim, action_dim),\n",
    "                nn.Tanh()\n",
    "            )\n",
    "        else:\n",
    "            self.actor = nn.Sequential(\n",
    "                nn.Linear(state_dim, hidden_dim),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(hidden_dim, hidden_dim),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(hidden_dim, action_dim),\n",
    "                nn.Softmax(dim=-1)\n",
    "            )\n",
    "        # critic\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "    def set_action_std(self, new_action_std):\n",
    "        if self.has_continuous_action_space:\n",
    "            self.action_var = torch.full(\n",
    "                (self.action_dim,), new_action_std * new_action_std).to(device)\n",
    "\n",
    "    def forward(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def act(self, state):\n",
    "        if self.has_continuous_action_space:\n",
    "            action_mean = self.actor(state)\n",
    "            cov_mat = torch.diag(self.action_var).unsqueeze(dim=0)\n",
    "            dist = MultivariateNormal(action_mean, cov_mat)\n",
    "        else:\n",
    "            action_probs = self.actor(state)\n",
    "            dist = Categorical(action_probs)\n",
    "\n",
    "        action = dist.sample()\n",
    "        action_logprob = dist.log_prob(action)\n",
    "        state_val = self.critic(state)\n",
    "\n",
    "        return action.detach(), action_logprob.detach(), state_val.detach()\n",
    "\n",
    "    def evaluate(self, state, action):\n",
    "        action_mean = self.actor(state)\n",
    "        action_var = self.action_var.expand_as(action_mean)\n",
    "        cov_mat = torch.diag_embed(action_var).to(device)\n",
    "        dist = MultivariateNormal(action_mean, cov_mat)\n",
    "        if self.action_dim == 1:\n",
    "            action = action.reshape(-1, self.action_dim)\n",
    "        action_logprobs = dist.log_prob(action)\n",
    "        dist_entropy = dist.entropy()\n",
    "        state_values = self.critic(state)\n",
    "\n",
    "        return action_logprobs, state_values, dist_entropy\n",
    "\n",
    "\n",
    "class PPO:\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim, replay_buffer, isStu=True, lr_actor=1e-3, lr_critic=1e-3, gamma=0.99, K_epochs=80, eps_clip=0.2, has_continuous_action_space=True, action_std_init=0.6):\n",
    "\n",
    "        self.has_continuous_action_space = has_continuous_action_space\n",
    "\n",
    "        self.action_std = action_std_init\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.eps_clip = eps_clip\n",
    "        self.K_epochs = K_epochs\n",
    "\n",
    "        self.buffer = replay_buffer\n",
    "        # print(self.buffer,replay_buffer)\n",
    "\n",
    "        self.policy = ActorCritic(state_dim, action_dim, hidden_dim,\n",
    "                                  has_continuous_action_space, action_std_init).to(device)\n",
    "        self.optimizer = torch.optim.Adam([\n",
    "            {'params': self.policy.actor.parameters(), 'lr': lr_actor},\n",
    "            {'params': self.policy.critic.parameters(), 'lr': lr_critic}\n",
    "        ])\n",
    "\n",
    "        self.policy_old = ActorCritic(\n",
    "            state_dim, action_dim, hidden_dim, has_continuous_action_space, action_std_init).to(device)\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "\n",
    "        self.MseLoss = nn.MSELoss()\n",
    "        \n",
    "        #Tensorboard Writer\n",
    "        if isStu:\n",
    "            self.tensorboard_writer=SummaryWriter('./runs/PPOee/Student/')\n",
    "        else:\n",
    "            self.tensorboard_writer=SummaryWriter('./runs/PPOee/Teacher/')\n",
    "        self.loss=[]\n",
    "        self.value_loss=[]\n",
    "        self.policy_entropy=[]\n",
    "        self.ptr=0\n",
    "\n",
    "    def set_action_std(self, new_action_std):\n",
    "        self.action_std = new_action_std\n",
    "        self.policy.set_action_std(new_action_std)\n",
    "        self.policy_old.set_action_std(new_action_std)\n",
    "\n",
    "    def decay_action_std(self, action_std_decay_rate, min_action_std):\n",
    "        self.action_std = self.action_std - action_std_decay_rate\n",
    "        self.action_std = round(self.action_std, 4)\n",
    "        if (self.action_std <= min_action_std):\n",
    "            self.action_std = min_action_std\n",
    "            print(\"setting actor output action_std to min_action_std : \",\n",
    "                  self.action_std)\n",
    "        else:\n",
    "            print(\"setting actor output action_std to : \", self.action_std)\n",
    "        self.set_action_std(self.action_std)\n",
    "\n",
    "    def select_action(self, state):\n",
    "        with torch.no_grad():\n",
    "            state = torch.FloatTensor(state).view(-1).to(device)\n",
    "            action, action_logprob, state_val = self.policy_old.act(state)\n",
    "\n",
    "        return action.detach().cpu().numpy().flatten(), action, action_logprob, state_val\n",
    "\n",
    "    def update(self, batch_size):\n",
    "        # 改成sample一些buffer\n",
    "        index = self.buffer.sample(batch_size, True)\n",
    "\n",
    "        # Monte Carlo estimate of returns\n",
    "        rewards = []\n",
    "        discounted_reward = 0\n",
    "        for reward, is_terminal in zip(reversed([self.buffer.reward[i] for i in index]), reversed([self.buffer.done[i] for i in index])):\n",
    "            if is_terminal:\n",
    "                discounted_reward = 0\n",
    "            discounted_reward = reward + (self.gamma * discounted_reward)\n",
    "            rewards.insert(0, discounted_reward)\n",
    "\n",
    "        # Normalizing the rewards\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32).to(device)\n",
    "        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-7)\n",
    "\n",
    "        old_states = self.buffer.state_from_index(index).to(device)\n",
    "        old_actions = torch.stack([self.buffer.action[i]\n",
    "                                  for i in index], dim=0).detach().to(device)\n",
    "        old_logprobs = torch.stack([self.buffer.logprobs[i]\n",
    "                                   for i in index], dim=0).detach().to(device)\n",
    "        old_state_values = torch.stack(\n",
    "            [self.buffer.state_value[i] for i in index], dim=0).detach().to(device)\n",
    "\n",
    "        # calculate advantages\n",
    "        advantages = rewards.detach() - old_state_values.detach()\n",
    "\n",
    "        # Optimize policy for K epochs\n",
    "        for _ in range(self.K_epochs):\n",
    "\n",
    "            # Evaluating old actions and values\n",
    "            logprobs, state_values, dist_entropy = self.policy.evaluate(\n",
    "                old_states, old_actions)\n",
    "\n",
    "            # match state_values tensor dimensions with rewards tensor\n",
    "            # state_values = torch.squeeze(state_values)\n",
    "\n",
    "            # Finding the ratio (pi_theta / pi_theta__old)\n",
    "            ratios = torch.exp(logprobs - old_logprobs.detach())\n",
    "\n",
    "            # Finding Surrogate Loss\n",
    "            surr1 = ratios * advantages\n",
    "            surr2 = torch.clamp(ratios, 1-self.eps_clip,\n",
    "                                1+self.eps_clip) * advantages\n",
    "\n",
    "            # final loss of clipped objective PPO\n",
    "            loss = -torch.min(surr1, surr2) + 0.5 * \\\n",
    "                self.MseLoss(state_values, rewards) - 0.01 * dist_entropy\n",
    "                \n",
    "            # self.loss.append(loss)\n",
    "            # self.value_loss.append(self.MseLoss(state_values, rewards))\n",
    "            # self.policy_entropy.append(dist_entropy)\n",
    "            print(self.MseLoss(state_values, rewards))\n",
    "            self.tensorboard_writer.add_scalar(\"Loss\", loss.mean(), self.ptr)\n",
    "            self.tensorboard_writer.add_scalar(\"Value Loss\", self.MseLoss(state_values, rewards), self.ptr)\n",
    "            self.tensorboard_writer.add_scalar(\"Entropy\", dist_entropy.mean(), self.ptr)\n",
    "            self.ptr+=1\n",
    "\n",
    "            # take gradient step\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.mean().backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "        # Copy new weights into old policy\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "\n",
    "        # clear buffer\n",
    "        # self.buffer.clear()\n",
    "        \n",
    "    def WriteTensorboard(self,logdir:str):\n",
    "        \n",
    "        for i in range(len(self.loss)):\n",
    "            self.tensorboard_writer.add_scalar(\"Loss\", self.loss[i], i)\n",
    "            self.tensorboard_writer.add_scalar(\"Value Loss\", self.value_loss[i], i)\n",
    "            self.tensorboard_writer.add_scalar(\"Entropy\", self.policy_entropy[i], i)\n",
    "        \n",
    "\n",
    "    def save(self, checkpoint_path):\n",
    "        torch.save(self.policy_old.state_dict(), checkpoint_path)\n",
    "\n",
    "    def load(self, checkpoint_path):\n",
    "        self.policy_old.load_state_dict(torch.load(\n",
    "            checkpoint_path, map_location=lambda storage, loc: storage))\n",
    "        self.policy.load_state_dict(torch.load(\n",
    "            checkpoint_path, map_location=lambda storage, loc: storage))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "class StudentAgent:\n",
    "    def __init__(self, state_dim, action_dim, env):\n",
    "        self.env = env\n",
    "        # self.replay_buffer=self.env.student_buffer\n",
    "        self.replay_buffer = env.student_buffer\n",
    "        self.model = PPO(state_dim, action_dim, 32, self.replay_buffer)\n",
    "\n",
    "    def generate_trajectory(self, step: int):\n",
    "        pb = tqdm(range(step))\n",
    "        num = 0\n",
    "        for i in pb:\n",
    "            s = self.env.reset()\n",
    "            d = False\n",
    "            while not d:\n",
    "                a, _, l, v = self.model.select_action(s)\n",
    "                s_, r, d, _ = self.env.step(a)\n",
    "                self.model.buffer.store(s, a, l, s_, r, v, d)\n",
    "                s = s_\n",
    "                num += 1\n",
    "            # pb.update()\n",
    "        print('生成', num, '条轨迹')\n",
    "\n",
    "    def train(self, total_timestep, batch_size):\n",
    "        pb = tqdm(range(total_timestep))\n",
    "        for i in pb:\n",
    "            self.model.update(batch_size)\n",
    "            pb.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Student(StudentAgent):\n",
    "    def __init__(self, state_dim, action_dim, env):\n",
    "        super().__init__(state_dim, action_dim, env)\n",
    "        \n",
    "    def generate_trajectory(self, step: int):\n",
    "        pb=tqdm(range(step))\n",
    "        for _ in pb:\n",
    "            s=self.env.reset()\n",
    "            d=False\n",
    "            while not d:\n",
    "                a,action,l,v=self.model.select_action(s)\n",
    "                s_,r,d,_=self.env.step(a)\n",
    "                self.model.buffer.store(s[-3:],a,l,s_,r,v,d)\n",
    "                s=s_\n",
    "            pb.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from CustomEnv import CustomEnv\n",
    "import network_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "History length: 10\n",
      "Features: ['sent latency inflation', 'latency ratio', 'send ratio']\n",
      "Getting min obs for ['sent latency inflation', 'latency ratio', 'send ratio']\n"
     ]
    }
   ],
   "source": [
    "env=CustomEnv('PccNs-v0',4096)\n",
    "sa=Student(np.prod(env.get_state_dim()),env.get_action_dim()[0],env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:01<00:00,  3.13it/s]\n"
     ]
    }
   ],
   "source": [
    "sa.generate_trajectory(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0456, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(1.0169, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9952, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9804, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9719, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9689, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9701, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9736, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9773, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9797, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9804, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9795, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9776, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9751, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9727, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9708, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9695, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9689, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9689, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9693, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9700, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9706, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9711, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9713, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9713, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9710, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9706, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9701, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9696, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9692, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9689, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9688, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9688, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9690, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9691, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9693, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9694, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9694, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9694, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9693, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9692, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9690, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9689, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9688, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9688, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9688, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9688, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9689, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9689, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9690, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9690, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9690, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9689, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9689, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9689, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9688, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9688, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9688, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9688, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9688, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9688, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9688, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9688, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9688, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9688, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9688, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9688, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9688, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9688, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9688, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9688, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9688, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9688, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9688, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9688, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9688, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9688, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9688, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9688, device='cuda:0', grad_fn=<MseLossBackward0>)\n",
      "tensor(0.9688, device='cuda:0', grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "sa.model.update(32)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
