{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "    def __init__(self,memsize,state_dim,action_dim):\n",
    "        self.memsize=memsize\n",
    "        self.cntr=0\n",
    "        self.s=np.zeros((self.memsize,state_dim))\n",
    "        self.a=np.zeros((self.memsize,action_dim))\n",
    "        self.r=np.zeros(self.memsize)\n",
    "        self.s_=np.zeros((self.memsize,state_dim))\n",
    "        self.d=np.zeros(self.memsize,dtype=np.bool8)\n",
    "        \n",
    "    def add(self,state,action,reward,next_state,done):\n",
    "        index=self.cntr%self.memsize\n",
    "        self.s[index]=state\n",
    "        self.a[index]=action\n",
    "        self.r[index]=reward\n",
    "        self.s_[index]=next_state\n",
    "        self.d[index]=done\n",
    "        self.cntr+=1\n",
    "        \n",
    "    def sample(self,batch_size):\n",
    "        maxmem=min(self.memsize,self.cntr)\n",
    "        index=np.random.choice(maxmem,batch_size,replace=True)\n",
    "        states=self.s[index]\n",
    "        actions=self.a[index]\n",
    "        rewards=self.r[index]\n",
    "        next_states=self.s_[index]\n",
    "        dones=self.d[index]\n",
    "        \n",
    "        return states,actions,rewards,next_states,dones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim,hidden_dim,max_action,max_log_std=2):\n",
    "        super(Actor,self).__init__()\n",
    "        self.fc1=nn.Linear(state_dim,hidden_dim)\n",
    "        self.fc2=nn.Linear(hidden_dim,hidden_dim)\n",
    "        self.mu_head=nn.Linear(hidden_dim,1)\n",
    "        self.log_std_head=nn.Linear(hidden_dim,1)\n",
    "        self.max_action=max_action\n",
    "        self.reparam_noise=1e-6\n",
    "        \n",
    "        self.max_log_std=max_log_std\n",
    "        \n",
    "    \n",
    "    def forward(self,x):\n",
    "        x=F.relu(self.fc1(x))\n",
    "        x=F.relu(self.fc2(x))\n",
    "        mu=self.mu_head(x)\n",
    "        log_std_head=self.log_std_head(x)\n",
    "        log_std_head=torch.clamp(log_std_head,self.reparam_noise,self.max_log_std)\n",
    "        return mu,log_std_head\n",
    "    \n",
    "    def select_action(self,state):\n",
    "        mu,log_std_head=self.forward(state)\n",
    "        probs=torch.distributions.Normal(mu,log_std_head)\n",
    "        action_=probs.sample()\n",
    "        action=torch.tanh(action_)*torch.tensor(self.max_action)\n",
    "        log_probs=probs.log_prob(action_)\n",
    "        log_probs-=torch.log(1-action.pow(2)+self.reparam_noise)\n",
    "        #print(log_probs)\n",
    "        log_probs=log_probs.sum(1,keepdim=True)\n",
    "        \n",
    "        return action,log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Q(nn.Module):\n",
    "    def __init__(self,state_dim,action_dim,hidden_dim):\n",
    "        super(Q,self).__init__()\n",
    "        self.q=nn.Sequential(\n",
    "            nn.Linear(state_dim+action_dim,hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim,hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim,1)\n",
    "        )\n",
    "        \n",
    "    def forward(self,state,action):\n",
    "        x=torch.cat([state,action],dim=-1)\n",
    "        return self.q(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self,state_dim,hidden_dim):\n",
    "        super(Critic,self).__init__()\n",
    "        self.critic=nn.Sequential(\n",
    "            nn.Linear(state_dim,hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim,hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim,1)\n",
    "        )\n",
    "        \n",
    "    def forward(self,state):\n",
    "        return self.critic(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构建SAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAC:\n",
    "    def __init__(self,state_dim,action_dim,hidden_dim,\n",
    "                 max_action,actor_lr=1e-3,value_lr=1e-3,\n",
    "                 q_lr=1e-3,gamma=0.99,tau=0.005,memory_size=8196,batch_size=64):\n",
    "        #读取参数\n",
    "        self.state_dim=state_dim\n",
    "        self.action_dim=action_dim\n",
    "        self.hidden_dim=hidden_dim\n",
    "        self.max_action=max_action\n",
    "        self.actor_lr=actor_lr\n",
    "        self.q_lr=q_lr\n",
    "        self.value_lr=value_lr\n",
    "        self.gamma=gamma\n",
    "        self.tau=tau\n",
    "        self.memory_size=memory_size\n",
    "        self.batch_size=batch_size\n",
    "        \n",
    "        #初始化各部件\n",
    "        ##回放缓存\n",
    "        self.replay_buffer=ReplayBuffer(self.memory_size,self.state_dim,self.action_dim)\n",
    "        ##策略网络\n",
    "        self.actor=Actor(self.state_dim,self.action_dim,self.hidden_dim,self.max_action)\n",
    "        #self.target_actor=Actor(self.state_dim,self.action_dim,self.hidden_dim,self.max_action)\n",
    "        self.actor_optimizer=optim.Adam(self.actor.parameters(),lr=actor_lr)\n",
    "        ##价值网络\n",
    "        self.value=Critic(self.state_dim,self.hidden_dim)\n",
    "        self.target_value=Critic(self.state_dim,self.hidden_dim)\n",
    "        self.value_optimizer=optim.Adam(self.value.parameters(),lr=value_lr)\n",
    "        ##动作价值网络\n",
    "        self.qvalue=Q(self.state_dim,self.action_dim,self.hidden_dim)\n",
    "        #self.target_qvalue=Q(self.state_dim,self.action_dim,self.hidden_dim)\n",
    "        self.qvalue_optimizer=optim.Adam(self.qvalue.parameters(),lr=q_lr)\n",
    "        \n",
    "    def store(self,state,action,reward,next_state,done):\n",
    "        self.replay_buffer.add(state,action,reward,next_state,done)\n",
    "    \n",
    "    def sample(self):\n",
    "        state,action,reward,next_state,done=self.replay_buffer.sample(self.batch_size)\n",
    "        state=torch.FloatTensor(state)\n",
    "        action=torch.FloatTensor(action)\n",
    "        reward=torch.FloatTensor(reward)\n",
    "        next_state=torch.FloatTensor(next_state)\n",
    "        done=torch.BoolTensor(done)\n",
    "        return state,action,reward,next_state,done\n",
    "    \n",
    "    def select_action(self,state):\n",
    "        state=torch.FloatTensor(state).unsqueeze(0)\n",
    "        action,_=self.actor.select_action(state)\n",
    "        return action.detach().numpy()[0]\n",
    "    \n",
    "    def update(self):\n",
    "        #采样\n",
    "        state,action,reward,next_state,done=self.sample()\n",
    "        \n",
    "        #计算alue值\n",
    "        value=self.value(state).view(-1)\n",
    "        next_value=self.target_value(next_state).view(-1)\n",
    "        \n",
    "        #选择动作\n",
    "        action_sampled,log_prob=self.actor.select_action(state)\n",
    "        log_prob=log_prob.view(-1)\n",
    "        \n",
    "        #计算q值\n",
    "        q_value=self.qvalue(state,action_sampled).view(-1)\n",
    "        \n",
    "        #更新value网络\n",
    "        self.value_optimizer.zero_grad()\n",
    "        target_value=q_value-log_prob\n",
    "        value_loss=0.5*F.mse_loss(value,target_value)\n",
    "        value_loss.backward(retain_graph=True)\n",
    "        self.value_optimizer.step()\n",
    "        \n",
    "        #更新Actor网络\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss=log_prob-q_value\n",
    "        actor_loss=torch.mean(actor_loss)\n",
    "        actor_loss.backward(retain_graph=True)\n",
    "        self.actor_optimizer.step()\n",
    "        \n",
    "        #更新Q网络\n",
    "        self.qvalue_optimizer.zero_grad()\n",
    "        q_hat=reward+self.gamma*next_value\n",
    "        q_value=self.qvalue(state,action)\n",
    "        q_loss=F.mse_loss(q_value.view(-1),q_hat)\n",
    "        q_loss.backward()\n",
    "        self.qvalue_optimizer.step()\n",
    "        \n",
    "        self.update_parameters()\n",
    "        \n",
    "    def update_parameters(self):\n",
    "        value_state_dict=dict(self.value.named_parameters())\n",
    "        target_value_state_dict=dict(self.target_value.named_parameters())\n",
    "        \n",
    "        for name in value_state_dict:\n",
    "            value_state_dict[name]=self.tau*value_state_dict[name].clone()+\\\n",
    "                (1-self.tau)*target_value_state_dict[name].clone()\n",
    "        self.target_value.load_state_dict(value_state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#初始化\n",
    "agent=SAC(5,1,32,2)\n",
    "#网络是否可以运行\n",
    "state=torch.randn(5)\n",
    "action,_=agent.actor(state)\n",
    "value=agent.value(state)\n",
    "qvalue=agent.qvalue(state,action)\n",
    "#优化器是否可运行\n",
    "##生成目标值\n",
    "state=torch.randn(5)\n",
    "action_,_=agent.actor(state)\n",
    "value_=agent.value(state)\n",
    "qvalue_=agent.qvalue(state,action_)\n",
    "##计算loss\n",
    "action_loss=F.mse_loss(action,action_)\n",
    "value_loss=F.mse_loss(value,value_)\n",
    "qvalue_loss=F.mse_loss(qvalue,qvalue_)\n",
    "##清空梯度->反向传播->优化\n",
    "action_loss.backward()\n",
    "agent.actor_optimizer.step()\n",
    "agent.actor_optimizer.zero_grad()\n",
    "value_loss.backward(retain_graph=True)\n",
    "agent.value_optimizer.step()\n",
    "agent.value_optimizer.zero_grad()\n",
    "qvalue_loss.backward()\n",
    "agent.qvalue_optimizer.step()\n",
    "agent.qvalue_optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试rb模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent=SAC(5,1,32,2)\n",
    "for _ in range(10000):\n",
    "    state=np.random.randn(5)\n",
    "    action=np.random.randn(1)\n",
    "    reward=np.random.randn(1)\n",
    "    next_state=np.random.randn(5)\n",
    "    done=np.random.randint(0,2)\n",
    "    agent.store(state,action,reward,next_state,done)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里输入需要维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac=Actor(5,32,1)\n",
    "action,log_probs=ac.select_action(torch.randn(5,5))\n",
    "log_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在gym环境中测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "env=gym.make('Pendulum-v0')\n",
    "state_dim=env.observation_space.shape[0]\n",
    "action_dim=env.action_space.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "需要用unsqueeze(0)将tensor升维"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent=SAC(state_dim,action_dim,hidden_dim=32,max_action=env.action_space.high.item())\n",
    "a=agent.select_action(torch.FloatTensor(env.reset()).unsqueeze(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actor网络中logProb似乎有些问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration： 0\n",
      "iteration： 10\n",
      "iteration： 20\n",
      "iteration： 30\n",
      "iteration： 40\n",
      "iteration： 50\n",
      "iteration： 60\n",
      "iteration： 70\n",
      "iteration： 80\n",
      "iteration： 90\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for it in range(100):\n",
    "    done=False\n",
    "    state=env.reset()\n",
    "    while not done:\n",
    "        action=agent.select_action(state)\n",
    "        state_,r,done,_=env.step(action)\n",
    "        agent.store(state,action,r,state_,done)\n",
    "        agent.update()\n",
    "        state=state_\n",
    "    if it%10==0:\n",
    "        print('iteration：',it)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gym环境跑通！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下一步开始适配Aurora环境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import network_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "History length: 10\n",
      "Features: ['sent latency inflation', 'latency ratio', 'send ratio']\n",
      "Getting min obs for ['sent latency inflation', 'latency ratio', 'send ratio']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/data/envs/IL/lib/python3.8/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "env=gym.make('PccNs-v0')\n",
    "state_dim=env.observation_space.shape[0]\n",
    "action_dim=env.action_space.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "AuroraAgent=SAC(state_dim,action_dim,32,env.action_space.high[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon=0.1\n",
    "for i in range(100):\n",
    "    done=False\n",
    "    s=env.reset()\n",
    "    while not done:\n",
    "        action=AuroraAgent.select_action(s)\n",
    "        s_,r,done,_=env.step(action)\n",
    "        AuroraAgent.store(s,action,r,s_,done)\n",
    "        AuroraAgent.update()\n",
    "        s=s_\n",
    "    if i%10==0:\n",
    "        print('iteration:',i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 8.0688076e+00,  9.8935059e+03,  2.9261279e+02,  9.9961920e+00,\n",
       "        5.3750400e+03,  3.5672318e+02, -6.4626765e-01,  8.5764717e+03,\n",
       "        3.8121045e+02,  1.3743095e+00,  2.1900139e+03,  1.8200874e+02,\n",
       "        6.1614542e+00,  8.8220762e+03,  8.8058185e+02, -7.2015804e-01,\n",
       "        2.1096001e+03,  9.1302472e+02, -7.2793168e-01,  6.4749634e+03,\n",
       "        2.8407443e+02,  5.0648036e+00,  1.6514847e+03,  3.1797876e+02,\n",
       "        2.9469061e+00,  6.3996619e+02,  8.0839709e+02,  7.6744790e+00,\n",
       "        6.8565776e+03,  4.6947217e+02], dtype=float32)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.sample()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
