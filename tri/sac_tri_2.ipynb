{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "    def __init__(self,memsize,state_dim,action_dim):\n",
    "        self.memsize=memsize\n",
    "        self.cntr=0\n",
    "        self.s=np.zeros((self.memsize,state_dim))\n",
    "        self.a=np.zeros((self.memsize,action_dim))\n",
    "        self.r=np.zeros(self.memsize)\n",
    "        self.s_=np.zeros((self.memsize,state_dim))\n",
    "        self.d=np.zeros(self.memsize,dtype=np.bool8)\n",
    "        \n",
    "    def add(self,state,action,reward,next_state,done):\n",
    "        index=self.cntr%self.memsize\n",
    "        self.s[index]=state\n",
    "        self.a[index]=action\n",
    "        self.r[index]=reward\n",
    "        self.s_[index]=next_state\n",
    "        self.d[index]=done\n",
    "        self.cntr+=1\n",
    "        \n",
    "    def sample(self,batch_size):\n",
    "        maxmem=min(self.memsize,self.cntr)\n",
    "        index=np.random.choice(maxmem,batch_size,replace=True)\n",
    "        states=self.s[index]\n",
    "        actions=self.a[index]\n",
    "        rewards=self.r[index]\n",
    "        next_states=self.s_[index]\n",
    "        dones=self.d[index]\n",
    "        \n",
    "        return states,actions,rewards,next_states,dones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim,hidden_dim,max_action,min_log_std=-20,max_log_std=2):\n",
    "        super(Actor,self).__init__()\n",
    "        self.fc1=nn.Linear(state_dim,hidden_dim)\n",
    "        self.fc2=nn.Linear(hidden_dim,hidden_dim)\n",
    "        self.mu_head=nn.Linear(hidden_dim,1)\n",
    "        self.log_std_head=nn.Linear(hidden_dim,1)\n",
    "        self.max_action=max_action\n",
    "        self.reparam_noise=1e-6\n",
    "        \n",
    "        self.min_log_std=min_log_std\n",
    "        self.max_log_std=max_log_std\n",
    "        \n",
    "    \n",
    "    def forward(self,x):\n",
    "        x=F.relu(self.fc1(x))\n",
    "        x=F.relu(self.fc2(x))\n",
    "        mu=self.mu_head(x)\n",
    "        log_std_head=self.log_std_head(x)\n",
    "        log_std_head=torch.clamp(log_std_head,self.min_log_std,self.max_log_std)\n",
    "        return mu,log_std_head\n",
    "    \n",
    "    def select_action(self,state):\n",
    "        mu,log_std_head=self.forward(state)\n",
    "        probs=torch.distributions.Normal(mu,log_std_head)\n",
    "        action_=probs.sample()\n",
    "        action=torch.tanh(action_)*torch.tensor(self.max_action)\n",
    "        log_probs=probs.log_prob(action_)\n",
    "        log_probs-=torch.log(1-action.pow(2)+self.reparam_noise)\n",
    "        print(log_probs)\n",
    "        log_probs=log_probs.sum(1,keepdim=True)\n",
    "        \n",
    "        return action,log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Q(nn.Module):\n",
    "    def __init__(self,state_dim,action_dim,hidden_dim):\n",
    "        super(Q,self).__init__()\n",
    "        self.q=nn.Sequential(\n",
    "            nn.Linear(state_dim+action_dim,hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim,hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim,1)\n",
    "        )\n",
    "        \n",
    "    def forward(self,state,action):\n",
    "        x=torch.cat([state,action],dim=-1)\n",
    "        return self.q(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self,state_dim,hidden_dim):\n",
    "        super(Critic,self).__init__()\n",
    "        self.critic=nn.Sequential(\n",
    "            nn.Linear(state_dim,hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim,hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim,1)\n",
    "        )\n",
    "        \n",
    "    def forward(self,state):\n",
    "        return self.critic(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rb=ReplayBuffer(10,5,2)\n",
    "for _ in range(32):\n",
    "    state=np.random.randn(5)\n",
    "    action=np.random.randn(2)\n",
    "    reward=np.random.randn(1)\n",
    "    next_state=np.random.randn(5)\n",
    "    done=np.random.randint(0,2)\n",
    "    rb.add(state,action,reward,next_state,done)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构建SAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAC:\n",
    "    def __init__(self,state_dim,action_dim,hidden_dim,\n",
    "                 max_action,actor_lr=1e-3,value_lr=1e-3,\n",
    "                 q_lr=1e-3,gamma=0.99,tau=0.005,memory_size=8196,batch_size=64):\n",
    "        #读取参数\n",
    "        self.state_dim=state_dim\n",
    "        self.action_dim=action_dim\n",
    "        self.hidden_dim=hidden_dim\n",
    "        self.max_action=max_action\n",
    "        self.actor_lr=actor_lr\n",
    "        self.q_lr=q_lr\n",
    "        self.value_lr=value_lr\n",
    "        self.gamma=gamma\n",
    "        self.tau=tau\n",
    "        self.memory_size=memory_size\n",
    "        self.batch_size=batch_size\n",
    "        \n",
    "        #初始化各部件\n",
    "        ##回放缓存\n",
    "        self.replay_buffer=ReplayBuffer(self.memory_size,self.state_dim,self.action_dim)\n",
    "        ##策略网络\n",
    "        self.actor=Actor(self.state_dim,self.action_dim,self.hidden_dim,self.max_action)\n",
    "        #self.target_actor=Actor(self.state_dim,self.action_dim,self.hidden_dim,self.max_action)\n",
    "        self.actor_optimizer=optim.Adam(self.actor.parameters(),lr=actor_lr)\n",
    "        ##价值网络\n",
    "        self.value=Critic(self.state_dim,self.hidden_dim)\n",
    "        self.target_value=Critic(self.state_dim,self.hidden_dim)\n",
    "        self.value_optimizer=optim.Adam(self.value.parameters(),lr=value_lr)\n",
    "        ##动作价值网络\n",
    "        self.qvalue=Q(self.state_dim,self.action_dim,self.hidden_dim)\n",
    "        #self.target_qvalue=Q(self.state_dim,self.action_dim,self.hidden_dim)\n",
    "        self.qvalue_optimizer=optim.Adam(self.qvalue.parameters(),lr=q_lr)\n",
    "        \n",
    "    def store(self,state,action,reward,next_state,done):\n",
    "        self.replay_buffer.add(state,action,reward,next_state,done)\n",
    "    \n",
    "    def sample(self):\n",
    "        state,action,reward,next_state,done=self.replay_buffer.sample(self.batch_size)\n",
    "        state=torch.FloatTensor(state)\n",
    "        action=torch.FloatTensor(action)\n",
    "        reward=torch.FloatTensor(reward)\n",
    "        next_state=torch.FloatTensor(next_state)\n",
    "        done=torch.BoolTensor(done)\n",
    "        return state,action,reward,next_state,done\n",
    "    \n",
    "    def select_action(self,state):\n",
    "        state=torch.FloatTensor(state)\n",
    "        return self.actor.select_action(state)\n",
    "    \n",
    "    def update(self):\n",
    "        #采样\n",
    "        state,action,reward,next_state,done=self.sample()\n",
    "        \n",
    "        #计算alue值\n",
    "        value=self.value(state).view(-1)\n",
    "        next_value=self.target_value(next_state).view(-1)\n",
    "        \n",
    "        #选择动作\n",
    "        action_sampled,log_prob=self.actor.select_action(state)\n",
    "        log_prob=log_prob.view(-1)\n",
    "        \n",
    "        #计算q值\n",
    "        q_value=self.qvalue(state,action_sampled).view(-1)\n",
    "        \n",
    "        #更新value网络\n",
    "        self.value_optimizer.zero_grad()\n",
    "        target_value=q_value-log_prob\n",
    "        value_loss=0.5*F.mse_loss(value,target_value)\n",
    "        value_loss.backward(retain_graph=True)\n",
    "        self.value_optimizer.step()\n",
    "        \n",
    "        #更新Actor网络\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss=log_prob-q_value\n",
    "        actor_loss=torch.mean(actor_loss)\n",
    "        actor_loss.backward(retain_graph=True)\n",
    "        self.actor_optimizer.step()\n",
    "        \n",
    "        #更新Q网络\n",
    "        self.qvalue_optimizer.zero_grad()\n",
    "        q_hat=reward+self.gamma*next_value\n",
    "        q_value=self.qvalue(state,action)\n",
    "        q_loss=F.mse_loss(q_value.view(-1),q_hat)\n",
    "        q_loss.backward()\n",
    "        self.qvalue_optimizer.step()\n",
    "        \n",
    "        self.update_parameters()\n",
    "        \n",
    "    def update_parameters(self):\n",
    "        value_state_dict=dict(self.value.named_parameters())\n",
    "        target_value_state_dict=dict(self.target_value.named_parameters())\n",
    "        \n",
    "        for name in value_state_dict:\n",
    "            value_state_dict[name]=self.tau*value_state_dict[name].clone()+\\\n",
    "                (1-self.tau)*target_value_state_dict[name].clone()\n",
    "        self.target_value.load_state_dict(value_state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#初始化\n",
    "agent=SAC(5,1,32,2)\n",
    "#网络是否可以运行\n",
    "state=torch.randn(5)\n",
    "action,_=agent.actor(state)\n",
    "value=agent.value(state)\n",
    "qvalue=agent.qvalue(state,action)\n",
    "#优化器是否可运行\n",
    "##生成目标值\n",
    "state=torch.randn(5)\n",
    "action_,_=agent.actor(state)\n",
    "value_=agent.value(state)\n",
    "qvalue_=agent.qvalue(state,action_)\n",
    "##计算loss\n",
    "action_loss=F.mse_loss(action,action_)\n",
    "value_loss=F.mse_loss(value,value_)\n",
    "qvalue_loss=F.mse_loss(qvalue,qvalue_)\n",
    "##清空梯度->反向传播->优化\n",
    "action_loss.backward()\n",
    "agent.actor_optimizer.step()\n",
    "agent.actor_optimizer.zero_grad()\n",
    "value_loss.backward(retain_graph=True)\n",
    "agent.value_optimizer.step()\n",
    "agent.value_optimizer.zero_grad()\n",
    "qvalue_loss.backward()\n",
    "agent.qvalue_optimizer.step()\n",
    "agent.qvalue_optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试rb模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent=SAC(5,1,32,2)\n",
    "for _ in range(10000):\n",
    "    state=np.random.randn(5)\n",
    "    action=np.random.randn(1)\n",
    "    reward=np.random.randn(1)\n",
    "    next_state=np.random.randn(5)\n",
    "    done=np.random.randint(0,2)\n",
    "    agent.store(state,action,reward,next_state,done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "state,action,reward,next_state,done=agent.sample()\n",
    "action,_=agent.actor.select_action(state)\n",
    "value=agent.value(state)\n",
    "qvalue=agent.qvalue(state,action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(100):\n",
    "    agent.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在gym环境中测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "env=gym.make('Pendulum-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dim=env.observation_space.shape[0]\n",
    "action_dim=env.action_space.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([nan], grad_fn=<SubBackward0>)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/tools/DR/MyProject/tri/sac_tri_2.ipynb 单元格 18\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/tools/DR/MyProject/tri/sac_tri_2.ipynb#X31sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m agent\u001b[39m=\u001b[39mSAC(state_dim,action_dim,hidden_dim\u001b[39m=\u001b[39m\u001b[39m32\u001b[39m,max_action\u001b[39m=\u001b[39menv\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39mhigh\u001b[39m.\u001b[39mitem())\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/tools/DR/MyProject/tri/sac_tri_2.ipynb#X31sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m a\u001b[39m=\u001b[39magent\u001b[39m.\u001b[39;49mselect_action(env\u001b[39m.\u001b[39;49mreset())\n",
      "\u001b[1;32m/home/tools/DR/MyProject/tri/sac_tri_2.ipynb 单元格 18\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tools/DR/MyProject/tri/sac_tri_2.ipynb#X31sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mselect_action\u001b[39m(\u001b[39mself\u001b[39m,state):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tools/DR/MyProject/tri/sac_tri_2.ipynb#X31sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m     state\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mFloatTensor(state)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/tools/DR/MyProject/tri/sac_tri_2.ipynb#X31sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mactor\u001b[39m.\u001b[39;49mselect_action(state)\n",
      "\u001b[1;32m/home/tools/DR/MyProject/tri/sac_tri_2.ipynb 单元格 18\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tools/DR/MyProject/tri/sac_tri_2.ipynb#X31sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m log_probs\u001b[39m-\u001b[39m\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mlog(\u001b[39m1\u001b[39m\u001b[39m-\u001b[39maction\u001b[39m.\u001b[39mpow(\u001b[39m2\u001b[39m)\u001b[39m+\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreparam_noise)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tools/DR/MyProject/tri/sac_tri_2.ipynb#X31sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39mprint\u001b[39m(log_probs)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/tools/DR/MyProject/tri/sac_tri_2.ipynb#X31sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m log_probs\u001b[39m=\u001b[39mlog_probs\u001b[39m.\u001b[39;49msum(\u001b[39m1\u001b[39;49m,keepdim\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tools/DR/MyProject/tri/sac_tri_2.ipynb#X31sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m \u001b[39mreturn\u001b[39;00m action,log_probs\n",
      "\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": [
    "agent=SAC(state_dim,action_dim,hidden_dim=32,max_action=env.action_space.high.item())\n",
    "a=agent.select_action(env.reset())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actor网络中logProb似乎有些问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for _ in range(100):\n",
    "    done=False\n",
    "    state=env.reset()\n",
    "    while not done:\n",
    "        action,_=agent.actor.select_action(state)\n",
    "        state_,r,done,_=env.step(action)\n",
    "        agent.store(state,action,r,next_state,done)\n",
    "        agent.update()\n",
    "        state=state_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
