{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 导入库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "from typing import Tuple\n",
    "from collections import namedtuple\n",
    "from torch.distributions import Normal\n",
    "\n",
    "Buffer_Capacity=8192\n",
    "min_Val=torch.tensor(1e-7).float()\n",
    "gradient_steps=1\n",
    "batch_size=128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.创建Actor&Critic网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim,hidden_dim,max_action,min_log_std=-20,max_log_std=2):\n",
    "        super(Actor,self).__init__()\n",
    "        self.fc1=nn.Linear(state_dim,hidden_dim)\n",
    "        self.fc2=nn.Linear(hidden_dim,hidden_dim)\n",
    "        self.mu_head=nn.Linear(hidden_dim,1)\n",
    "        self.log_std_head=nn.Linear(hidden_dim,1)\n",
    "        self.max_action=max_action\n",
    "        \n",
    "        self.min_log_std=min_log_std\n",
    "        self.max_log_std=max_log_std\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x=F.relu(self.fc1(x))\n",
    "        x=F.relu(self.fc2(x))\n",
    "        mu=self.mu_head(x)\n",
    "        log_std_head=F.relu(self.log_std_head(x))\n",
    "        log_std_head=torch.clamp(log_std_head,self.min_log_std,self.max_log_std)\n",
    "        return mu,log_std_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Q(nn.Module):\n",
    "    def __init__(self,state_dim,action_dim,hidden_dim):\n",
    "        super(Q,self).__init__()\n",
    "        self.q=nn.Sequential(\n",
    "            nn.Linear(state_dim+action_dim,hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim,hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim,1)\n",
    "        )\n",
    "        \n",
    "    def forward(self,state,action):\n",
    "        x=torch.cat([state,action],dim=-1)\n",
    "        return self.q(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self,state_dim,hidden_dim):\n",
    "        super(Critic,self).__init__()\n",
    "        self.critic=nn.Sequential(\n",
    "            nn.Linear(state_dim,hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim,hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim,1)\n",
    "        )\n",
    "        \n",
    "    def forward(self,state):\n",
    "        return self.critic(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.Actor&Critic网络测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state: tensor([-1.0421,  0.5407, -0.0612,  1.6671, -0.7643])\n",
      "action: (tensor([-0.1809], grad_fn=<ViewBackward0>), tensor([0.], grad_fn=<ClampBackward1>))\n",
      "state value: tensor([-0.1126], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "state=torch.randn(5)\n",
    "print('state:',state)\n",
    "\n",
    "actor=Actor(5,32,1)\n",
    "critic=Critic(5,32)\n",
    "q=Q(5,2,32)\n",
    "\n",
    "action=actor.forward(state)\n",
    "value=critic.forward(state)\n",
    "#qvalue=q.forward(state,action)\n",
    "\n",
    "print('action:',action)\n",
    "print('state value:',value)\n",
    "#print('qvalue:',qvalue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. 构建SAC类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition', ['s', 'a', 'r', 's_', 'd'])\n",
    "class SAC:\n",
    "    def __init__(self,state_dim,action_dim,hidden_dim,max_action,device,actor_lr=1e-3,critic_lr=1e-3,gamma=0.99,tau=0.005,alpha=0.2):\n",
    "        \n",
    "        self.device=device\n",
    "        \n",
    "        self.policy_net=Actor(state_dim,action_dim,hidden_dim,max_action).to(self.device)\n",
    "        self.value_net=Critic(state_dim,hidden_dim).to(self.device)\n",
    "        self.Q_net=Q(state_dim,action_dim,hidden_dim).to(self.device)\n",
    "        self.target_value_net=Critic(state_dim,hidden_dim).to(self.device)\n",
    "        \n",
    "        self.replay_buffer=[Transition]*Buffer_Capacity\n",
    "        self.policy_optimizer=torch.optim.Adam(self.policy_net.parameters(),lr=actor_lr)\n",
    "        self.value_optimizer=torch.optim.Adam(self.value_net.parameters(),lr=critic_lr)\n",
    "        self.Q_optimizer=torch.optim.Adam(self.Q_net.parameters(),lr=critic_lr)\n",
    "        self.num_transition=0\n",
    "        self.num_training=1\n",
    "        \n",
    "        self.value_criterion=nn.MSELoss()\n",
    "        self.Q_criterion=nn.MSELoss()\n",
    "        \n",
    "        for target_param,param in zip(self.target_value_net.parameters(),self.value_net.parameters()):\n",
    "            target_param.data.copy_(param.data)\n",
    "            \n",
    "        self.gamma=gamma\n",
    "        self.tau=tau\n",
    "        self.alpha=alpha\n",
    "            \n",
    "    def select_action(self,state):\n",
    "        state=torch.FloatTensor(state).to(self.device)\n",
    "        mu,log_sigma=self.policy_net(state)\n",
    "        sigma=torch.exp(log_sigma)\n",
    "        dist=Normal(mu,sigma)\n",
    "        z=dist.sample()\n",
    "        action=torch.tanh(z).detach().cpu().numpy()\n",
    "        return action\n",
    "    \n",
    "    def store(self,s,a,r,s_,d):\n",
    "        index=self.num_transition%Buffer_Capacity\n",
    "        transition=Transition(s,a,r,s_,d)\n",
    "        self.replay_buffer[index]=transition\n",
    "        self.num_transition+=1\n",
    "        \n",
    "    def get_action_log_prob(self,state):\n",
    "        batch_mu,batch_log_sigma=self.policy_net(state)\n",
    "        batch_sigma=torch.exp(batch_log_sigma)\n",
    "        dist=Normal(batch_mu,batch_sigma)\n",
    "        z=dist.sample()\n",
    "        action=torch.tanh(z)\n",
    "        log_prob=dist.log_prob(z)-torch.log(1-action.pow(2)+min_Val)\n",
    "        return action,log_prob,z,batch_mu,batch_log_sigma\n",
    "    \n",
    "    def update(self):\n",
    "        if self.num_training%500==0:\n",
    "            print(\"Training ...{}\",format(self.num_training))\n",
    "        s=torch.tensor([t.s for t in self.replay_buffer]).float().to(self.device)\n",
    "        a=torch.tensor([t.a for t in self.replay_buffer]).float().to(self.device)\n",
    "        r=torch.tensor([t.r for t in self.replay_buffer]).float().to(self.device)\n",
    "        s_=torch.tensor([t.s_ for t in self.replay_buffer]).float().to(self.device)\n",
    "        d=torch.tensor([t.d for t in self.replay_buffer]).int().to(self.device)\n",
    "        \n",
    "        for _ in range(gradient_steps):\n",
    "            index=np.random.choice(range(Buffer_Capacity),batch_size,replace=False)\n",
    "            bn_s=s[index]\n",
    "            bn_a=a[index].reshape(-1,1)\n",
    "            bn_r=r[index].reshape(-1,1)\n",
    "            bn_s_ = s_[index]\n",
    "            bn_d = d[index].reshape(-1, 1)\n",
    "            \n",
    "            target_value=self.target_value_net(bn_s_)\n",
    "            next_q_value=bn_r+(1-bn_d)*self.gamma*target_value\n",
    "            \n",
    "            expected_value=self.value_net(bn_s)\n",
    "            print(bn_s.shape,bn_a.shape)\n",
    "            expected_Q=self.Q_net(bn_s,bn_a)\n",
    "            \n",
    "            sample_action,log_prob,z,batch_mu,batch_log_sigma=self.get_action_log_prob(bn_s)\n",
    "            expected_new_Q=self.Q_net(bn_s,sample_action)\n",
    "            next_value=expected_new_Q-log_prob\n",
    "            \n",
    "            v_loss=self.value_criterion(expected_value,next_value.detach())\n",
    "            v_loss=v_loss.mean()\n",
    "            \n",
    "            Q_loss=self.Q_criterion(expected_Q,next_q_value.detach())\n",
    "            Q_loss = Q_loss.mean()\n",
    "\n",
    "            log_policy_target = expected_new_Q - expected_value\n",
    "\n",
    "            pi_loss = log_prob * (log_prob- log_policy_target).detach()\n",
    "            pi_loss = pi_loss.mean()\n",
    "            \n",
    "            self.value_optimizer.zero_grad()\n",
    "            v_loss.backward(retain_graph=True)\n",
    "            nn.utils.clip_grad_norm_(self.value_net.parameters(), 0.5)\n",
    "            self.value_optimizer.step()\n",
    "\n",
    "            self.Q_optimizer.zero_grad()\n",
    "            Q_loss.backward(retain_graph = True)\n",
    "            nn.utils.clip_grad_norm_(self.Q_net.parameters(), 0.5)\n",
    "            self.Q_optimizer.step()\n",
    "\n",
    "            self.policy_optimizer.zero_grad()\n",
    "            pi_loss.backward(retain_graph = True)\n",
    "            nn.utils.clip_grad_norm_(self.policy_net.parameters(), 0.5)\n",
    "            self.policy_optimizer.step()\n",
    "            \n",
    "            for target_param, param in zip(self.Target_value_net.parameters(), self.value_net.parameters()):\n",
    "                target_param.data.copy_(target_param * (1 - self.tau) + param * self.tau)\n",
    "\n",
    "            self.num_training += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. 测试SAC模块"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.1 初始化SAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state: [-1.60781207  1.21274204  0.19062339  0.48187438 -1.79303331]\n"
     ]
    }
   ],
   "source": [
    "state=np.random.randn(5)\n",
    "print('state:',state)\n",
    "sac=SAC(state_dim=5,action_dim=1,hidden_dim=32,max_action=1,device='cuda')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.2 测试select_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9627316]\n"
     ]
    }
   ],
   "source": [
    "act=sac.select_action(state)\n",
    "print(act)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.3 测试Replay_Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(10000):\n",
    "    s=np.random.randn(5)\n",
    "    a=sac.select_action(s)\n",
    "    r=np.random.randn(1)\n",
    "    s_=np.random.randn(5)\n",
    "    d=np.random.randint(0,1)\n",
    "    sac.store(s,a,r,s_,d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.4.1 测试update的replay模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 5]) torch.Size([128, 1])\n",
      "<class 'torch.Tensor'>\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Found dtype Double but expected Float",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/tools/DR/MyProject/tri/sac.ipynb 单元格 19\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/tools/DR/MyProject/tri/sac.ipynb#X24sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m sac\u001b[39m.\u001b[39;49mupdate()\n",
      "\u001b[1;32m/home/tools/DR/MyProject/tri/sac.ipynb 单元格 19\u001b[0m line \u001b[0;36m9\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tools/DR/MyProject/tri/sac.ipynb#X24sZmlsZQ%3D%3D?line=95'>96</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalue_optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/tools/DR/MyProject/tri/sac.ipynb#X24sZmlsZQ%3D%3D?line=97'>98</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mQ_optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/tools/DR/MyProject/tri/sac.ipynb#X24sZmlsZQ%3D%3D?line=98'>99</a>\u001b[0m Q_loss\u001b[39m.\u001b[39;49mbackward(retain_graph \u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/tools/DR/MyProject/tri/sac.ipynb#X24sZmlsZQ%3D%3D?line=99'>100</a>\u001b[0m nn\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mclip_grad_norm_(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mQ_net\u001b[39m.\u001b[39mparameters(), \u001b[39m0.5\u001b[39m)\n\u001b[1;32m    <a href='vscode-notebook-cell:/home/tools/DR/MyProject/tri/sac.ipynb#X24sZmlsZQ%3D%3D?line=100'>101</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mQ_optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m/home/data/envs/IL/lib/python3.8/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    493\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    494\u001b[0m )\n",
      "File \u001b[0;32m/home/data/envs/IL/lib/python3.8/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m     tensors,\n\u001b[1;32m    253\u001b[0m     grad_tensors_,\n\u001b[1;32m    254\u001b[0m     retain_graph,\n\u001b[1;32m    255\u001b[0m     create_graph,\n\u001b[1;32m    256\u001b[0m     inputs,\n\u001b[1;32m    257\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    258\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    259\u001b[0m )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Found dtype Double but expected Float"
     ]
    }
   ],
   "source": [
    "sac.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "index=np.random.choice(Buffer_Capacity,2,replace=False)\n",
    "s=torch.tensor([t.s for t in sac.replay_buffer]).float()\n",
    "a=torch.tensor([t.a for t in sac.replay_buffer]).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.9975],\n",
       "        [-1.0000]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1830],\n",
       "        [0.2313]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q=Q(5,1,32)\n",
    "q(s[index],a[index])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
