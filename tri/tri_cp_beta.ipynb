{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions.categorical import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOmemory:\n",
    "    def __init__(self, mini_batch_size):\n",
    "        self.states = []  # 状态\n",
    "        self.actions = []  # 实际采取的动作\n",
    "        self.probs = []  # 动作概率\n",
    "        self.vals = []  # critic输出的状态值\n",
    "        self.rewards = []  # 奖励\n",
    "        self.dones = []  # 结束标志\n",
    "\n",
    "        self.mini_batch_size = mini_batch_size  # minibatch的大小\n",
    "\n",
    "    def sample(self):\n",
    "        n_states = len(self.states)  # memory记录数量=20\n",
    "        batch_start = np.arange(0, n_states, self.mini_batch_size)  # 每个batch开始的位置[0,5,10,15]\n",
    "        indices = np.arange(n_states, dtype=np.int64)  # 记录编号[0,1,2....19]\n",
    "        np.random.shuffle(indices)  # 打乱编号顺序[3,1,9,11....18]\n",
    "        mini_batches = [indices[i:i + self.mini_batch_size] for i in batch_start]  # 生成4个minibatch，每个minibatch记录乱序且不重复\n",
    "\n",
    "        return np.array(self.states), np.array(self.actions), np.array(self.probs), \\\n",
    "               np.array(self.vals), np.array(self.rewards), np.array(self.dones), mini_batches\n",
    "\n",
    "    # 每一步都存储trace到memory\n",
    "    def push(self, state, action, prob, val, reward, done):\n",
    "        self.states.append(state)\n",
    "        self.actions.append(action)\n",
    "        self.probs.append(prob)\n",
    "        self.vals.append(val)\n",
    "        self.rewards.append(reward)\n",
    "        self.dones.append(done)\n",
    "\n",
    "    # 固定步长更新完网络后清空memory\n",
    "    def clear(self):\n",
    "        self.states = []\n",
    "        self.actions = []\n",
    "        self.probs = []\n",
    "        self.vals = []\n",
    "        self.rewards = []\n",
    "        self.dones = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actor:policy network\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, n_states, n_actions, hidden_dim):\n",
    "        super(Actor, self).__init__()\n",
    "\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(n_states, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim, n_actions),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "\n",
    "    def forward(self, state):\n",
    "        dist = self.actor(state)\n",
    "        dist = Categorical(dist)\n",
    "        entropy = dist.entropy()\n",
    "        return dist, entropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __inti__(self,n_states,hidden_dim):\n",
    "        super(Critic,self).__init__()\n",
    "        self.critic=nn.Sequential(\n",
    "            nn.Linear(n_states,hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim,hidden_dim),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_dim,1)\n",
    "        )\n",
    "        \n",
    "    def forward(self,state):\n",
    "        value=self.critic(state)\n",
    "        return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self,n_states,n_actions,cfg):\n",
    "        self.gamma = cfg.gamma\n",
    "        self.policy_clip = cfg.policy_clip\n",
    "        self.n_epochs = cfg.n_epochs\n",
    "        self.gae_lambda = cfg.gae_lambda\n",
    "        self.device = cfg.device\n",
    "\n",
    "        self.actor=Actor(n_states,n_actions,cfg.hidden_dim).to(self.device)\n",
    "        self.critic=Critic(n_states,cfg.hidden_dim).to(self.device)\n",
    "        self.actor_optimizer=optim.Adam(self.actor.parameters(),lr=cfg.actor_lr)\n",
    "        self.critic_optimizer=optim.Adam(self.critic.parameters(),lr=cfg.critic_lr)\n",
    "        \n",
    "        self.memory=PPOmemory(cfg.mini_batch_size)\n",
    "        self.loss=0\n",
    "        \n",
    "        def choose_action(self,state):\n",
    "            state=torch.tensor(state,dtype=torch.float).to(self.device)\n",
    "            dist,entropy=self.actor(state)\n",
    "            value=self.critic(state)\n",
    "            action=dist.sample()\n",
    "            prob=torch.squeeze(dist.log_probs(action)).item()\n",
    "            action=torch.squeeze(action).item()\n",
    "            value=torch.squeeze(value).item()\n",
    "            return action,prob,value\n",
    "        \n",
    "        def learn(self):\n",
    "            for _ in range(self.n_epochs):\n",
    "                state_arr, action_arr, old_prob_arr, vals_arr, reward_arr, dones_arr, batches = self.memory.sample()\n",
    "                values = vals_arr[:]\n",
    "                \n",
    "            #计算GAE\n",
    "            advantage=np.zeros(len(reward_arr),dtype=np.float32)\n",
    "            for t in range(len(reward_arr)-1):\n",
    "                discount=1\n",
    "                a_t=0\n",
    "                for k in range(t,len(reward_arr)):\n",
    "                    a_t+=discount*(reward_arr[k]+self.gamma*values[k+1]*(1-int(dones_arr[k]))-values[k])\n",
    "                    discount+=self.gamma*self.gae_lambda\n",
    "                advantage[t]=a_t\n",
    "            advantage=torch.tensor(advantage).to(self.device)\n",
    "            \n",
    "            \n",
    "            values=torch.tensor(values).to(self.device)\n",
    "            for batch in batches:\n",
    "                states=torch.tensor(state_arr[batch],dtype=torch.float).to(self.device)\n",
    "                old_probs=torch.tensor(old_prob_arr[batch],dtype=torch.float).to(self.device)\n",
    "                actions=torch.tensor(action_arr[batch],dtype=torch.float).to(self.device)\n",
    "                \n",
    "                dist,entropy=self.actor(states)\n",
    "                critic_value=torch.squeeze(self.critic(states))\n",
    "                new_probs=dist.log_prob(actions)\n",
    "                prob_ratio=new_probs.exp()/old_probs.exp()\n",
    "                \n",
    "                #计算策略梯度\n",
    "                weighted_probs=advantage[batch]*prob_ratio\n",
    "                weighted_clipped_probs=torch.clamp(prob_ratio,1-self.policy_clip,1+self.policy_clip)*advantage[batch]\n",
    "                actor_loss=torch.min(weighted_probs,weighted_clipped_probs).mean()\n",
    "                \n",
    "                # critic_loss\n",
    "                returns = advantage[batch] + values[batch]\n",
    "                critic_loss = (returns - critic_value) ** 2\n",
    "                critic_loss = critic_loss.mean()\n",
    "\n",
    "                # 更新\n",
    "                entropy_loss = entropy.mean()\n",
    "                total_loss = actor_loss + 0.5 * critic_loss - entropy_loss * 0.01\n",
    "                self.loss = total_loss\n",
    "                self.actor_optimizer.zero_grad()\n",
    "                self.critic_optimizer.zero_grad()\n",
    "                total_loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.actor.parameters(), 0.5)\n",
    "                torch.nn.utils.clip_grad_norm_(self.critic.parameters(), 0.5)\n",
    "                self.actor_optimizer.step()\n",
    "                self.critic_optimizer.step()\n",
    "        self.memory.clear()\n",
    "        return self.loss\n",
    "    def save(self, path):\n",
    "        actor_checkpoint = os.path.join(path, 'ppo_actor.pt')\n",
    "        critic_checkpoint = os.path.join(path, 'ppo_critic.pt')\n",
    "        torch.save(self.actor.state_dict(), actor_checkpoint)\n",
    "        torch.save(self.critic.state_dict(), critic_checkpoint)\n",
    "\n",
    "    def load(self, path):\n",
    "        actor_checkpoint = os.path.join(path, 'ppo_actor.pt')\n",
    "        critic_checkpoint = os.path.join(path, 'ppo_critic.pt')\n",
    "        self.actor.load_state_dict(torch.load(actor_checkpoint))\n",
    "        self.critic.load_state_dict(torch.load(critic_checkpoint))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_args():\n",
    "    parser = argparse.ArgumentParser(description=\"hyper parameters\")\n",
    "    parser.add_argument('--algo_name', default='PPO', type=str, help=\"name of algorithm\")\n",
    "    parser.add_argument('--env_name', default='CartPole-v1', type=str, help=\"name of environment\")\n",
    "    parser.add_argument('--train_eps', default=200, type=int, help=\"episodes of training\")\n",
    "    parser.add_argument('--test_eps', default=20, type=int, help=\"episodes of testing\")\n",
    "    parser.add_argument('--gamma', default=0.99, type=float, help=\"discounted factor\")\n",
    "    parser.add_argument('--mini_batch_size', default=5, type=int, help='mini batch size')\n",
    "    parser.add_argument('--n_epochs', default=4, type=int, help='update number')\n",
    "    parser.add_argument('--actor_lr', default=0.0003, type=float, help=\"learning rate of actor net\")\n",
    "    parser.add_argument('--critic_lr', default=0.0003, type=float, help=\"learning rate of critic net\")\n",
    "    parser.add_argument('--gae_lambda', default=0.95, type=float, help='GAE lambda')\n",
    "    parser.add_argument('--policy_clip', default=0.2, type=float, help='policy clip')\n",
    "    parser.add_argument('-batch_size', default=20, type=int, help='batch size')\n",
    "    parser.add_argument('--hidden_dim', default=256, type=int, help='hidden dim')\n",
    "    parser.add_argument('--device', default='cpu', type=str, help=\"cpu or cuda\")\n",
    "    args = parser.parse_args()\n",
    "    return args\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
