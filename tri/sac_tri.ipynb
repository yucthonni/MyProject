{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ReplayBuffer():\n",
    "    def __init__(self):\n",
    "        self.s=[]\n",
    "        self.a=[]\n",
    "        self.r=[]\n",
    "        self.s_=[]\n",
    "        self.d=[]\n",
    "    def add(self,state,action,reward,next_state,done):\n",
    "        self.s.append(state)\n",
    "        self.a.append(action)\n",
    "        self.r.append(reward)\n",
    "        self.s_.append(next_state)\n",
    "        self.d.append(done)\n",
    "        \n",
    "    def sample(self,batch_size):\n",
    "        index=np.random.choice(len(self.s),batch_size,replace=True)\n",
    "        states=[self.s[i] for i in index]\n",
    "        actions=[self.a[i] for i in index]\n",
    "        rewards=[self.r[i] for i in index]\n",
    "        next_states=[self.s_[i] for i in index]\n",
    "        dones=[self.d[i] for i in index]\n",
    "        \n",
    "        return states,actions,rewards,next_states,dones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim,max_action):\n",
    "        super(Actor, self).__init__()\n",
    "        self.max_action=max_action\n",
    "        self.fc1 = nn.Linear(state_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc3 = nn.Linear(256, action_dim)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return torch.tanh(self.fc3(x))*self.max_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim + action_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc3 = nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        x = torch.cat([state, action], dim=-1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SAC(object):\n",
    "    def __init__(self, state_dim, action_dim, max_action, gamma=0.99,alpha=0.2):\n",
    "        self.actor = Actor(state_dim, action_dim,max_action)\n",
    "        self.critic = Critic(state_dim, action_dim)\n",
    "\n",
    "        self.target_entropy = -action_dim\n",
    "        self.log_alpha = torch.zeros(1, requires_grad=True)\n",
    "        self.alpha = self.log_alpha.exp()\n",
    "\n",
    "        self.optimizer_actor = torch.optim.Adam(self.actor.parameters(), lr=3e-4)\n",
    "        self.optimizer_critic = torch.optim.Adam(self.critic.parameters(), lr=3e-4)\n",
    "        \n",
    "        self.gamma=gamma\n",
    "        self.alpha=alpha\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state = torch.FloatTensor(state).unsqueeze(0)\n",
    "        action = self.actor(state).detach().numpy()[0]\n",
    "        return action\n",
    "    \n",
    "\n",
    "    def update(self, replay_buffer, batch_size=100):\n",
    "        # Sample a batch of transitions from replay buffer\n",
    "        state, action, reward, next_state, done = replay_buffer.sample(batch_size)\n",
    "        \n",
    "        state=torch.FloatTensor(state)\n",
    "        action=torch.FloatTensor(action)\n",
    "        reward=torch.FloatTensor(reward)\n",
    "        next_state=torch.FloatTensor(next_state)\n",
    "        done=torch.FloatTensor(done)\n",
    "\n",
    "        # Compute the target Q-value\n",
    "        with torch.no_grad():\n",
    "            next_action = self.actor(next_state)\n",
    "            next_action=torch.FloatTensor(next_action)\n",
    "            target_Q = self.critic(next_state, next_action)\n",
    "            target_Q = reward + (1 - done) * self.gamma * (target_Q + self.alpha * next_action.pow(2).sum(dim=1))\n",
    "\n",
    "        # Get current Q-value estimates\n",
    "        current_Q = self.critic(state, action)\n",
    "\n",
    "        # Compute critic loss\n",
    "        critic_loss = F.mse_loss(current_Q, target_Q) + F.mse_loss(current_Q, target_Q)+1e-7\n",
    "\n",
    "        # Compute actor loss\n",
    "        actor_loss = -self.critic(state, self.actor(state)).mean() + self.alpha * (self.actor(state).pow(2).sum(dim=1) + self.target_entropy).mean()\n",
    "\n",
    "        # Optimize the actor and critic\n",
    "        self.optimizer_actor.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.optimizer_actor.step()\n",
    "\n",
    "        self.optimizer_critic.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.optimizer_critic.step()\n",
    "\n",
    "        # Update alpha\n",
    "        '''alpha_loss = -(self.log_alpha * (actor_loss + self.target_entropy).detach()).mean()\n",
    "        self.optimizer_alpha.zero_grad()\n",
    "        alpha_loss.backward()\n",
    "        self.optimizer_alpha.step()'''\n",
    "\n",
    "        self.alpha = self.log_alpha.exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "gamma = 0.99\n",
    "tau = 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "env=gym.make('Pendulum-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "done=False\n",
    "for _ in range(100):\n",
    "    for _ in range(200):\n",
    "        s=env.reset()\n",
    "        a=env.action_space.sample()\n",
    "        s,r,done,_=env.step(a)\n",
    "        env.render()\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dim=env.observation_space.shape[0]\n",
    "action_dim=env.action_space.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize SAC agent\n",
    "sac = SAC(state_dim, action_dim,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "rb=ReplayBuffer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}/200 0.0\n",
      "{}/200 1.0\n"
     ]
    }
   ],
   "source": [
    "rewards=[]\n",
    "for i in range(1000):\n",
    "    s=env.reset()\n",
    "    done=False\n",
    "    er=0\n",
    "    while not done:\n",
    "        a=sac.select_action(s)\n",
    "        s_,r,done,_=env.step(a)\n",
    "        rb.add(s,a,r,s_,done)\n",
    "        er+=r\n",
    "        s=s_\n",
    "    rewards.append(er)\n",
    "    if i%500==0:\n",
    "        print('{}/200',i/500)\n",
    "reward_before=np.mean(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "s,a,r,s_,d=rb.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 1)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s[0].reshape(-1,1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state,action,reward,next_state,done=rb.sample(6)\n",
    "next_state=torch.FloatTensor(next_state)\n",
    "next_action=sac.actor(next_state)\n",
    "next_action=torch.FloatTensor(next_action)\n",
    "print(next_state,next_action)\n",
    "x=torch.cat([next_state,next_action],dim=-1)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19347/3032939781.py:43: UserWarning: Using a target size (torch.Size([10000, 10000])) that is different to the input size (torch.Size([10000, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  critic_loss = F.mse_loss(current_Q, target_Q) + F.mse_loss(current_Q, target_Q)+1e-7\n"
     ]
    }
   ],
   "source": [
    "for _ in range(100):\n",
    "    sac.update(rb,10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards=[]\n",
    "for _ in range(1000):\n",
    "    s=env.reset()\n",
    "    done=False\n",
    "    er=0\n",
    "    while not done:\n",
    "        a=sac.select_action(s)\n",
    "        s_,r,done,_=env.step(a)\n",
    "        rb.add(s,a,r,s_,done)\n",
    "        er+=r\n",
    "        s=s_\n",
    "    rewards.append(er)\n",
    "reward_after=np.mean(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1555.4407565465579"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward_after"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
